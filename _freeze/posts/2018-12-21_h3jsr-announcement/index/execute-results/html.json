{
  "hash": "6f4901bde55d6342b82ed72bf3a34acd",
  "result": {
    "markdown": "---\ntitle: \"New R package: h3jsr\"\ndescription: \"Probably should have learned more JavaScript instead >.>\"\ndate: \"2018-12-21\"\ncategories: [R, H3, JavaScript, DGGS]\nimage: h3jsr-announcementplot-1.png\nexecute:\n    eval: false\n---\n\n\n## Background\n\nDiscrete Global Grids! [They're pretty cool](http://geoawesomeness.com/discrete-global-grid-system-dggs-new-reference-system/), and slowly starting to catch on. Google's been plugging away at [S2](https://github.com/google/s2geometry) for a while now, and Uber recently released [H3](https://github.com/uber/h3). Both libraries are open-sourced and have their own sets of interesting features, but don't seem to have found their way into traditional GIS software yet, so you need some coding skill to access them.\n\nI could see some interesting potential use cases for H3 as soon as I read the documentation, so I was super keen to start playing with it ASAP. There were some barriers between me and all the hexagons I could eat though, so I had to do a little work first.\n\nMy process here was basically:\n\n-   realise that no R bindings were available for `H3` (apart from [this attempt](https://github.com/scottmmjackson/h3r), which appears to be slightly abandoned and doesn't work in Windows), sulk a little\n-   realise there's a transpiled version, [`h3-js`](https://github.com/uber/h3-js), and that [V8](https://github.com/jeroen/V8) is a thing (hot damn!)\n-   spend a Saturday morning figuring out how to get `h3-js` bundled into a v8 session\n-   realise I now have to learn some damn JavaScript; spend Saturday afternoon on codecademy\n-   briefly ponder whether six (6) hours of JS experience is enough to get by\n-   proceed anyway because I've got this far and f\\*&k imposter syndrome, right? Right.\n-   ...?\n-   profit!\n\n`h3jsr` is [now available from GitHub](https://github.com/obrl-soil/h3jsr). I'm feeling pretty good about it.\n\n## Y tho\n\nRight now my own applications for this package are nice data aggregation and pretty maps. That might seem basic, but that does seem to be [all that Uber are using it for themselves](https://www.youtube.com/watch?v=ay2uwtRO3QE) so far, and its solved some substantial business problems.\n\n## Performance\n\nYou'll be able to do a fair bit with this package so long as you think ahead. The most important thing to remember is that every call to a `h3jsr` function involves casting data into a JS environment via JSON, and that eats time. Aim to feed as much data into one function call as possible - use lists, vectors or dataframes as input wherever you can, don't try and iterate over individual geometries. Bear in mind that there's an upper limit to what V8 can transfer in one hit.\n\n## Demo time\n\nI did the bulk of the work on this package back in July, and then idly tinkered with it while failing to complete this post - my examples were boring! Luckily the Uber Open Summit 2018 happened not long ago and as part of it, `h3-js` dev Nick Rabinowitz ran a great [live tutorial](https://youtu.be/BsMIrBHLfLE) on [Suitability Analysis using h3-js and Mapbox GL JS](https://beta.observablehq.com/@nrabinowitz/h3-tutorial-suitability-analysis). Attempting a rebuild in R seems like a good way to demonstrate key functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(geojsonsf)\nlibrary(tidyverse)\nlibrary(ggspatial)\nlibrary(raster)\nlibrary(sf)\nlibrary(h3jsr)\noptions(stringsAsFactors = FALSE)\n```\n:::\n\n\nAll the tutorial inputs are github gists, so I can download and convert them to `sf` data frames like so:\n\nOakland crime reports, last 90 days. Source: data.oaklandnet.com:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrime_90_days <- httr::GET('https://gist.githubusercontent.com/nrabinowitz/d3a5ca3e3e40727595dd137b65058c76/raw/f5ef0fed8972d04a27727ebb50e065265e2d853f/oakland_crime_90days.json') %>%\n  httr::content() %>%\n  fromJSON() %>%\n  sf::st_as_sf(., coords = c('lng', 'lat'), crs = 4326) # JSON's always 4326\n\nhead(crime_90_days)\n```\n:::\n\n\n``` r\n## Simple feature collection with 6 features and 1 field\n## geometry type:  POINT\n## dimension:      XY\n## bbox:           xmin: -122.2758 ymin: 37.75606 xmax: -122.1889 ymax: 37.81339\n## epsg (SRID):    4326\n## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n##                   type                   geometry\n## 1            VANDALISM POINT (-122.2655 37.81339)\n## 2              ASSAULT  POINT (-122.2758 37.7969)\n## 3        THEFT/LARCENY POINT (-122.2026 37.75606)\n## 4              ROBBERY POINT (-122.2352 37.78423)\n## 5 DISTURBING THE PEACE POINT (-122.1889 37.79133)\n## 6            VANDALISM  POINT (-122.219 37.78628)\n```\n\nOakland public school locations. Source: data.oaklandnet.com\n\n\n::: {.cell}\n\n```{.r .cell-code}\npublic_schools <- httr::GET('https://gist.githubusercontent.com/nrabinowitz/d3a5ca3e3e40727595dd137b65058c76/raw/babf7357f15c99a1b2a507a33d332a4a87b7df8d/public_schools.json') %>%\n  httr::content() %>%\n  fromJSON() %>%\n  sf::st_as_sf(., coords = c('lng', 'lat'), crs = 4326)\n\nhead(public_schools)\n```\n:::\n\n\n    ## Simple feature collection with 6 features and 1 field\n    ## geometry type:  POINT\n    ## dimension:      XY\n    ## bbox:           xmin: -122.2869 ymin: 37.74664 xmax: -122.1656 ymax: 37.813\n    ## epsg (SRID):    4326\n    ## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n    ##         type                   geometry\n    ## 1    Charter POINT (-122.1849 37.79886)\n    ## 2    Charter  POINT (-122.225 37.77617)\n    ## 3     Middle POINT (-122.1656 37.74664)\n    ## 4       High   POINT (-122.2869 37.813)\n    ## 5 Elementary   POINT (-122.23 37.77982)\n    ## 6 Elementary POINT (-122.2371 37.80036)\n\nBART station locations. Source: bart.gov. This is GeoJSON data, not straight JSON, so note how the import proccess is a little different.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbart_stations <- httr::GET('https://gist.githubusercontent.com/nrabinowitz/d3a5ca3e3e40727595dd137b65058c76/raw/8f1a3e30113472404feebc288e83688a6d5cf33d/bart.json') %>%\n  httr::content() %>%\n  geojson_sf()\n\nhead(bart_stations[, 1])\n```\n:::\n\n\n    ## Simple feature collection with 6 features and 1 field\n    ## geometry type:  POINT\n    ## dimension:      XYZ\n    ## bbox:           xmin: -122.4475 ymin: 37.72158 xmax: -122.2686 ymax: 37.8528\n    ## epsg (SRID):    4326\n    ## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n    ##                                  name                       geometry\n    ## 1 12th St. Oakland City Center (12TH) POINT Z (-122.2715 37.80377 0)\n    ## 2             16th St. Mission (16TH) POINT Z (-122.4197 37.76506 0)\n    ## 3             19th St. Oakland (19TH) POINT Z (-122.2686 37.80835 0)\n    ## 4             24th St. Mission (24TH) POINT Z (-122.4181 37.75247 0)\n    ## 5                        Ashby (ASHB)  POINT Z (-122.2701 37.8528 0)\n    ## 6                  Balboa Park (BALB) POINT Z (-122.4475 37.72158 0)\n\nTravel times from Oakland to downtown SF by census tract. Source: movement.uber.com\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsf_travel_times <- httr::GET('https://gist.githubusercontent.com/nrabinowitz/d3a5ca3e3e40727595dd137b65058c76/raw/657a9f3b64fedc718c3882cd4adc645ac0b4cfc5/oakland_travel_times.json') %>%\n  httr::content() %>%\n  geojson_sf()\n\nhead(sf_travel_times)\n```\n:::\n\n\n    ## Simple feature collection with 6 features and 3 fields\n    ## geometry type:  MULTIPOLYGON\n    ## dimension:      XY\n    ## bbox:           xmin: -122.3049 ymin: 37.74276 xmax: -122.1595 ymax: 37.84773\n    ## epsg (SRID):    4326\n    ## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n    ##   MOVEMENT_ID                                DISPLAY_NAME travelTime\n    ## 1          46   500 Chester Street, West Oakland, Oakland        708\n    ## 2          47             9700 Birch Street, Cox, Oakland       1575\n    ## 3          58        5600 Genoa Street, Santa Fe, Oakland       1015\n    ## 4          98  500 10th Street, Downtown Oakland, Oakland        826\n    ## 5          99 2400 19th Avenue, Highland Terrace, Oakland       1166\n    ## 6         151  500 20th Street, Downtown Oakland, Oakland        908\n    ##                         geometry\n    ## 1 MULTIPOLYGON (((-122.304 37...\n    ## 2 MULTIPOLYGON (((-122.1725 3...\n    ## 3 MULTIPOLYGON (((-122.2779 3...\n    ## 4 MULTIPOLYGON (((-122.2796 3...\n    ## 5 MULTIPOLYGON (((-122.238 37...\n    ## 6 MULTIPOLYGON (((-122.276 37...\n\nOakland points of interest. Source: uber.com/local\n\n\n::: {.cell}\n\n```{.r .cell-code}\npois <- httr::GET('https://gist.githubusercontent.com/nrabinowitz/d3a5ca3e3e40727595dd137b65058c76/raw/ded89c2acef426fe3ee59b05096ed1baecf02090/oakland-poi.json') %>%\n  httr::content()  %>%\n  fromJSON() %>%\n  sf::st_as_sf(., coords = c('lng', 'lat'), crs = 4326) %>%\n  dplyr::filter(type %in% c('Cafes', 'Places to eat', 'Restaurant'))\n\nhead(pois)\n```\n:::\n\n\n    ## Simple feature collection with 6 features and 1 field\n    ## geometry type:  POINT\n    ## dimension:      XY\n    ## bbox:           xmin: -122.2763 ymin: 37.77091 xmax: -122.211 ymax: 37.83476\n    ## epsg (SRID):    4326\n    ## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n    ##         type                   geometry\n    ## 1 Restaurant  POINT (-122.2567 37.8281)\n    ## 2 Restaurant POINT (-122.2632 37.83476)\n    ## 3 Restaurant POINT (-122.2705 37.80706)\n    ## 4 Restaurant  POINT (-122.211 37.77091)\n    ## 5 Restaurant POINT (-122.2763 37.79486)\n    ## 6 Restaurant POINT (-122.2437 37.81062)\n\nThat's a lot of messy data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncropper <- st_bbox(c('xmin' = -122.35, 'ymin' = 37.75,\n                     'xmax' = -122.20, 'ymax' = 37.85), crs = st_crs(4326))\nplot_these <- lapply(list(sf_travel_times, crime_90_days, public_schools,\n                          bart_stations, pois), function(x) {\n                            sf::st_crop(x, cropper)\n                            })\nggplot() +\n  layer_spatial(plot_these[[1]], aes(fill = travelTime), alpha = 0.9,\n                show.legend = FALSE) +\n  scale_fill_viridis_c() +\n  layer_spatial(plot_these[[2]], col = 'red',     pch = 20) +\n  layer_spatial(plot_these[[3]], col = 'cyan',    pch = 19) +\n  layer_spatial(plot_these[[4]], col = 'yellow',  pch = 18, size = 2) + \n  layer_spatial(plot_these[[5]], col = 'magenta', pch = 17) +\n  theme_minimal() +\n  ggtitle('H3 Suitability analysis', subtitle = 'Unprocessed input data')\n```\n:::\n\n\n![](h3jsr-announcementplot1-1.png){fig-alt=\"Map of imported datasets in their native spatial formats\" fig-align=\"center\" width=\"80%\"}\n\nTime to make some sense of it.\n\nWe'll take each of the raw data layers we're bringing in and convert them to hexagon layers. Each layer will be a map of H3 index to some value normalized between zero and 1 with this helper function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalise_layer <- function(layer = NULL, b0 = FALSE) {\n  dplyr::filter(layer, !is.na(h3)) %>%\n    dplyr::group_by(h3) %>%\n    dplyr::summarise('weight' = sum(weight, na.rm = TRUE)) %>%\n    dplyr::mutate(norm = if (b0) {\n      scales::rescale(weight, to = c(0, 1), from =  c(0, max(weight, na.rm = TRUE)))\n      } else { scales::rescale(weight, to = c(0, 1)) }) # from = range(x)\n}\n```\n:::\n\n\nAnalysis is being conducted at four of H3's 15 resolution levels - 7-10 - so each data layer must be binned and normalised four times. This is where `purrr` can be handy.\n\nFor crime, the output layer is just a normalised count of incidents per hex.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrime_hexes <- point_to_h3(crime_90_days, seq(7, 10)) %>%\n  purrr::map(., function(h3) {\n   dat <- data.frame('h3' = h3, 'weight' = 1L, \n                     stringsAsFactors = FALSE) %>%\n     normalise_layer()\n  })\nhead(crime_hexes[['h3_resolution_7']])\n```\n:::\n\n\n    ## # A tibble: 6 x 3\n    ##   h3              weight     norm\n    ##   <chr>            <int>    <dbl>\n    ## 1 872830802ffffff      5 0.000486\n    ## 2 872830810ffffff   2060 1       \n    ## 3 872830811ffffff    311 0.149   \n    ## 4 872830812ffffff    575 0.278   \n    ## 5 872830813ffffff    535 0.258   \n    ## 6 872830814ffffff    145 0.0686\n\nFor schools, there's a bit of buffering added so that addresses adjacent to those containing a school are given some weight.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschool_hexes <- point_to_h3(public_schools, seq(7, 10)) %>%\n  purrr::map(., function(h3) {\n    # returns 7 addresses - input and neighbours \n    near_school <- get_kring(h3, 1) \n    near_wts <- c(1, rep(0.5, 6)) # surrounds are worth half\n    # combine and normalise\n    dat <- purrr::map_dfr(near_school, function(h3) {\n      data.frame('h3' = h3, 'weight' = near_wts, stringsAsFactors = FALSE)\n    }) %>%\n      normalise_layer()\n  })\nhead(school_hexes[[1]])\n```\n:::\n\n\n    ## # A tibble: 6 x 3\n    ##   h3              weight   norm\n    ##   <chr>            <dbl>  <dbl>\n    ## 1 872830802ffffff    2.5 0.0556\n    ## 2 872830806ffffff    1   0.0139\n    ## 3 872830810ffffff   25.5 0.694 \n    ## 4 872830811ffffff   14.5 0.389 \n    ## 5 872830812ffffff   23.5 0.639 \n    ## 6 872830813ffffff   22.5 0.611\n\nFor BART stations, the buffering is more sophisticated, with a smooth decay function implemented. This does a better job of preserving the area of influence around a station across different H3 resolutions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm_to_radius <- function(km, res) {\n  floor(km / res_length(res, units = 'km'))\n}\n\nbart_hexes <- point_to_h3(bart_stations, seq(7, 10)) %>%\n  purrr::map2(., seq(7,10), function(h3, res) {\n    d <- km_to_radius(1, res)\n    near_bart <- get_kring_list(sort(unique(h3)), d)\n    # weights are the same for every feature so just make a template\n    near_wts <- purrr::map(near_bart[1], function(feature) {\n      purrr::map2(feature, seq_along(feature), function(ring, step) {\n        wt <- 1 - step * 1 / (d + 1)\n        rep(wt, length(ring))\n      })\n    }) %>% unlist()\n    purrr::map(near_bart, unlist) %>%\n      purrr::map_dfr(., function(x) {\n        data.frame('h3' = x, 'weight' = near_wts, stringsAsFactors = FALSE)\n        }) %>%\n      normalise_layer() \n      })\nhead(bart_hexes[[1]])\n```\n:::\n\n\n    ## # A tibble: 6 x 3\n    ##   h3              weight  norm\n    ##   <chr>            <dbl> <dbl>\n    ## 1 872830810ffffff      0   0.5\n    ## 2 872830813ffffff      0   0.5\n    ## 3 872830815ffffff      0   0.5\n    ## 4 872830828ffffff      0   0.5\n    ## 5 87283082affffff      0   0.5\n    ## 6 87283082cffffff      0   0.5\n\nFor travel time, we find all of the intersecting H3 addresses for each polygon in `sf_travel_times` before assigning them weights based on the travelTime attribute. The center of a given address must intersect the polygon before it can be returned. Weights are negative as lower travel times are better.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntravel_hexes <- purrr::map(seq(7, 10), function(res) {\n  dat <- polyfill(sf_travel_times, res, simple = FALSE) \n  dat <- purrr::map2_dfr(as.list(dat$travelTime), \n                         dat$h3_polyfillers, \n                         function(x, y) {\n                           data.frame('h3' = y, 'weight' = 1/x)\n    }) %>%\n    normalise_layer(., TRUE)\n})\nhead(travel_hexes[[1]])\n```\n:::\n\n\n    ## # A tibble: 6 x 3\n    ##   h3               weight  norm\n    ##   <chr>             <dbl> <dbl>\n    ## 1 872830802ffffff 0.00151 0.802\n    ## 2 872830810ffffff 0.00110 0.586\n    ## 3 872830811ffffff 0.00188 1    \n    ## 4 872830812ffffff 0.00127 0.673\n    ## 5 872830813ffffff 0.00137 0.727\n    ## 6 872830815ffffff 0.00129 0.686\n\nLastly, points of interest are tallied, just as crime was.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfood_hexes <- point_to_h3(pois, seq(7, 10)) %>%\n  purrr::map(., function(h3) {\n   dat <- data.frame('h3' = h3, 'weight' = 1L, \n                     stringsAsFactors = FALSE) %>%\n     normalise_layer()\n  })\nhead(food_hexes[[1]])\n```\n:::\n\n\n    ## # A tibble: 6 x 3\n    ##   h3              weight   norm\n    ##   <chr>            <int>  <dbl>\n    ## 1 872830810ffffff    851 1     \n    ## 2 872830811ffffff     16 0.0176\n    ## 3 872830812ffffff    407 0.478 \n    ## 4 872830813ffffff    331 0.388 \n    ## 5 872830814ffffff    173 0.202 \n    ## 6 872830816ffffff     98 0.114\n\nPhew! Now we can plug the data together to get some overall weights. For each resolution, the following code adds up the normalised weights all of the 'good' layers, and then subtracts crime.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# arrange data by resolution instead of theme\ndatnm <- c('crime', 'school', 'bart', 'travel', 'food')\ndat <- list(crime_hexes, school_hexes, bart_hexes,\n                  travel_hexes, food_hexes) %>%\n  purrr::transpose() %>%\n  purrr::map(., setNames, datnm)\n\nfinal_surfaces <- purrr::map(dat, function(res) {\n  # rename cols for nicer joins\n  purrr::map2(res, datnm, function(x,y) {\n    dplyr::rename_at(x, vars(norm), funs(paste0('n_', y))) %>%\n      dplyr::select(-weight)\n  }) %>%\n    # condense inputs\n    purrr::reduce(., full_join, by = 'h3') %>%\n    replace(is.na(.), 0) %>%\n    rowwise() %>%\n    dplyr::mutate(weight = \n                    sum(c(n_school, n_bart, n_travel, n_food)) - n_crime) %>%\n    ungroup() %>%\n    dplyr::select(h3, weight) %>%\n    normalise_layer() %>%\n    h3_to_polygon(., simple = FALSE)\n})\nhead(final_surfaces[[2]])\n```\n:::\n\n\n    ## Simple feature collection with 6 features and 4 fields\n    ## geometry type:  POLYGON\n    ## dimension:      XY\n    ## bbox:           xmin: -122.3869 ymin: 37.79247 xmax: -122.3053 ymax: 37.8162\n    ## epsg (SRID):    4326\n    ## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n    ##      weight       norm      h3_address h3_resolution\n    ## 1 0.0000000 0.02495816 882830801bfffff             8\n    ## 2 0.8024133 0.45593409 8828308021fffff             8\n    ## 3 0.8024133 0.45593409 8828308023fffff             8\n    ## 4 0.8024133 0.45593409 8828308025fffff             8\n    ## 5 1.0000000 0.56205787 8828308027fffff             8\n    ## 6 0.8024133 0.45593409 882830802bfffff             8\n    ##                         geometry\n    ## 1 POLYGON ((-122.3796 37.7924...\n    ## 2 POLYGON ((-122.3202 37.7985...\n    ## 3 POLYGON ((-122.3276 37.8044...\n    ## 4 POLYGON ((-122.3098 37.8009...\n    ## 5 POLYGON ((-122.3172 37.8068...\n    ## 6 POLYGON ((-122.3306 37.7961...\n\nAll that and I still can't give you a cool interactive map like the source Observable notebook, but my plot below matches up nicely. Winner!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncropper <- st_bbox(c('xmin' = -122.35, 'ymin' = 37.75,\n                     'xmax' = -122.20, 'ymax' = 37.85), crs = st_crs(4326))\nplot_that <- st_crop(do.call(rbind, final_surfaces), cropper)\n\nggplot() +\n  layer_spatial(plot_that, aes(fill = norm), alpha = 0.9, col = NA) +\n  facet_wrap(. ~ h3_resolution)  +\n  scale_fill_gradientn(colors = c('#ffffD9', '#50BAC3', '#1A468A')) +\n  scale_x_continuous(breaks = c(-122.32, -122.27, -122.22)) +\n  ggtitle('H3 Suitability Analysis', \n          subtitle = 'Liveability at four resolutions') +\n  labs(fill = 'Weight') +\n  theme_minimal()\n```\n:::\n\n\n![](h3jsr-announcementplot-1.png){fig-alt=\"A map of the hexified and unified data, faceted over four increasingly detailed resolution levels\" fig-align=\"center\" width=\"80%\"}\n\nIts nice to confirm that `h3jsr` does what it oughta, but my major take-home from this is that I really need to learn me some geospatial JS. The code is much more concise than the R equivalent. In fact, the above was worse before I took the time to revamp a couple of my wrapper functions. The JS is also very fast, and the interactive maps are a real winner. I know I could probably do something here with `mapview` or better yet, `mapdeck` and Shiny, but that would be even more code (not to mention getting it to work properly on my current blog setup, which I'm not sure is possible...). So I'm very impressed.\n\nAnyway, there it is, feel free to experiment with the package, and I welcome PRs that will speed it up or improve useability. Its been a worthwhile learning exercise, and I'm using `h3jsr` fairly often at work. I'm pretty sure that an `Rcpp`-powered version of this that hooks into original-flavour H3 will be much, much faster, though, so I don't think this package will go to CRAN.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}