[
  {
    "objectID": "posts/2023-03-28_obsidian-setup-2023/index.html",
    "href": "posts/2023-03-28_obsidian-setup-2023/index.html",
    "title": "Research Notes Setup 2023",
    "section": "",
    "text": "Iâ€™ve seen a few posts and videos lately1 about how various people manage the avalanche of technical literature coming their way2. There are many helpful tips available in those posts, but nothing quite fit my own situation and needs end-to-end. So: this post is about how I currently manage a natural resource science-heavy reference library and associated knowledge generation infrastructure.\nTo be clear, I donâ€™t necessarily love this setup. Its the best Iâ€™ve managed so far though, so hopefully writing about what works and what could be better will be inspirational.\n\n\nI am:\n\na translational research worker with high anxiety and a short attention span\n\nI need/want:\n\nA setup that is compatible with my work IT environment but not dependent on it, because that is safe\nA setup that is relatively easy to upgrade and extend and generally futz around with, because that is fun.\n\n\n\n\nI am currently using:\n\nReference manager: Zotero, with the following extensions (â€˜add-onsâ€™):\n\nZotfile, for reference and source document management. Offers a one-click solution for moving a file into a designated spot and renaming it according to a given pattern.\nBetter Bibtex, which assigns unique IDs to Zotero references and automates their export to a *.bib file that can be picked up by other software that doesnâ€™t have close integration with Zotero.\n\nNotes manager: Obsidian, with the following extensions (â€˜community pluginsâ€™):\n\nCitations: picks up the *.bib file that Better Bibtex puts down, and allows me to generate a markdown-format literature note for each reference with a matching name, some nice metadata, and a pre-formatted space all ready for me to add my notes.\nAdmonition: (optional extra) makes it easy to add relative dates by e.g.Â typing in, e.g.Â @today, which then becomes a link named after todayâ€™s date.\nProjects: (optional extra) This plugin is very new, so maybe not super stable in the medium term, but I really like the ability to create table, kanban, gallery and calendar views of my notes. Its currently about as Notion or Anytype as you can get with markdown powering everything.\n\nStorage: Office 365 OneDrive. Its whatever, I get it for free through work, just about any cloud storage environment could be dropped in its place.\nAuthoring: Sometimes Obsidian, sometimes Quarto in RStudio, sometimes even (gasp) MS Word. The important part is that only Word and RStudio have full Zotero integration; other apps have to interact with it via the aforementioned *.bib file.\n\n\n\nIâ€™ve installed Zotero the usual way on a Windows machine. I have then added Zotfile, and set up my preferences as follows:\n\nGeneral Settings &gt; Location of Files: I have chosen â€˜Custom Locationâ€™, and instructed it to place any new linked file attachments in a particular folder in my OneDrive. I will call it âœ¨TheÂ Hoardâœ¨ going forward. The only important thing about âœ¨TheÂ Hoardâœ¨ is that it lives inside an Obsidian Vault. A Vault is just another folder, with some extra bits inside.\nRenaming Rules: This determines what the names of linked file attachments look like once theyâ€™re moved into âœ¨TheÂ Hoardâœ¨. I use the template {%a}_{%y}_{%t} , which is first author surname, year, title, separated by underscores. I have also ticked â€œchange to lower caseâ€, â€œtruncate title after . or : or ?â€3 , set â€œmaximum length of titleâ€ to 160, and maximum number of authors to 1. As such most of my linked files start out looking like, e.g.Â â€œdarwin_1859_on_the_origin_of_species.pdfâ€.\n\nI canâ€™t leave well enough alone, so I tend to do a bit more manual tweaking ğŸ™„. I add an â€œitem typeâ€ slug like _PAPER_ or _THESIS_ , I remove filler words like â€˜andâ€™, â€˜theâ€™ and â€˜novelâ€™ from the title, and I change underscores in titles to dashes, after Jenny Bryanâ€™s advice. They come out looking a bit like, e.g.Â â€œDarwin_1859_BOOK_origin-species-1e.pdfâ€. I do this because I want âœ¨TheÂ Hoardâœ¨ to be navigable on its own.\n\n\n\n\n\n\n\n\nTip\n\n\n\nMaximum length of title becomes important if you use cloud storage. OneDriveâ€™s maximum path length is 400 characters and some client software makes a fuss past 260 characters, so if your Hoard folder is nested deeply you have less to work with.\n\n\n\nAdvanced Settings: I expanded the â€˜Only work with the following filetypesâ€™ option to cover some extra attachment file types I often use, e.g.Â MS Office formats, common image formats, ebooks, markdown, html and zip files. If you donâ€™t do this, automated move-and-rename wonâ€™t work properly.\n\n\n\n\n\n\n\nNote\n\n\n\nThis option seems to mostly exist to protect against unintended consequences from enabling another setting in General Options - â€˜source folder for attaching new filesâ€™ grabs the most recently modified file in a nominated folder. This seems like more trouble than its worth, so if you actually use it, please explainâ€¦\n\n\nContinuing, I have added Better Bibtex, and left all the defaults except:\n\nAutomatic export: â€œWhen Idleâ€, delayed for 60 seconds. This is a compromise; I work with a lot of grey literature and other Old Stuff that has to be manually entered into Zotero. The default setting was fast enough that I was running into freeze-ups and sync errors because I was still editing a reference while BB was trying to re-export my whole library. Waiting longer means I sometimes have to manually trigger an export to make a new reference available outside Zotero.\nThe plugin is instructed to export my library to a *.bib file in Better Bibtex format that sits in the same folder as âœ¨TheÂ Hoardâœ¨.\n\n\n\n\nThese arenâ€™t important, but they are nice.\n\nZutilo - allows mass editing of reference item tags and attachment paths\nScite - scite.ai integration - citation records with context! I like what theyâ€™re doing. The service is still a bit incomplete in my main research fields, but improving all the time. One day Iâ€™ll convince my work to pay for itâ€¦\nWord integration - eh, why wouldnâ€™t you?\nMdnotes - not actually using this at the moment, but if you have a lot of notes already in Zotero and you want to move them into Obsidian, it would be very helpful.\n\n\n\n\nI have installed the software and set up a vault in my OneDrive called, imaginatively, Work_Stuff.\n\n\n\n\n\n\nImportant\n\n\n\nYou may have to pay for a commercial license to use Obsidian. Iâ€™m not currently as I donâ€™t qualify according to the EULA (NZ Crown Research Institutes are a weird public/private monstrosity, but fundamentally Iâ€™m employed as a non-profit government researcher). You may or may not have to depending on your circumstances, look into it and do the right thing.\n\n\nâœ¨TheÂ Hoardâœ¨ lives in a subfolder of Work_Stuff, as mentioned.\nI have then installed the Citations community plugin, with the following settings:\n\nCitation database format: BibLatex\nCitation database path: points at the *.bib file exported from Zotero by BB.\nLiterature Note folder: do what you want here. I started a specific Notes - sources folder inside the Vault (at the same level as âœ¨TheÂ Hoardâœ¨). You can get more specific and organise by project, or just store your notes in a big pile in the vaultâ€™s root directory if youâ€™re a filthy zoomer who doesnâ€™t understand file systems, Iâ€™m not judging4.\n\n\n\n\nMy templating choices are a little off-label, and this is where things get brittle. The key is that I use the â€˜Short Titleâ€™ field in Zotero to store the name of the file attachment I want to write about, sans file extension5. This means I can do two things: generate a note with a title that matches its source, and link to the source within the note template. In the main Zotero panel, an entry for Darwin (1859) might look like\n\n\n\n\n\nand in the item details:\n\n\n\n\n\nNow, in the Citations plugin settings,\n\nLiterature note title template: {titleShort}\nLiterature note content template:\n\ntitle: \"{{title}}\"\nfirst author: {{entry.data.creators.author.[0].lastName}}, {{entry.data.creators.author.[0].firstName}}\nyear: {{year}}\ndoi: {{DOI}}\ntype: {{entry.type}}\n---\nref::[Open reference in Zotero]({{zoteroSelectURI}})\npdf::[[{{titleShort}}.pdf|Open stored document]]\n***\n\n***\n## Abstract\n{{abstract}}\n\n## Notes\n\n## Cites\n\n## Cited by\n\n## Why I read this\n\n\n\n\n\n\n\nImportant\n\n\n\nThe double quotes around {title} stop the metadata breaking if you have any YAML-reserved characters in the article title, like :.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI put tags in between the two *** rows, which render as a horizontal line in reading view.\n\n\nExample:\n\n\n\nI might even give this reference the attention it deserves at some pointâ€¦\n\n\nYouâ€™ll note the â€˜refâ€™ and â€˜pdfâ€™ rows use some syntax from the Obsidian Dataview plugin. This is a bit of future-proofing in case I get into using that extension - no immediate plans.\nI didnâ€™t originally have a â€˜why I read thisâ€™ line in the template, but Iâ€™ve found that useful for adding context to my notes. A lot of my sources have multiple potential applications so its become worth noting what I cared about each time I accessed them. This also helps me avoid falling down a rabbit-hole of chasing interesting facts when I donâ€™t have time - I can add a â€˜come back to thisâ€™ tag and just let it go otherwise, no pressure.\n\n\n\nI set up a Project over my Notes - sources folder, with a Table view. I pull in a few metadata fields that make finding things easy - note filename, first author, title, article type, year. At present thereâ€™s no proper date handling so year just looks like a large number, but it still works for sort/filter. An extra nice thing about Projects is that it forces me to make sure my note metadata is actually correctly populated and formatted6.\n\n\n\nNow you know what Iâ€™ve been reading lately ğŸ˜\n\n\n\n\n\n\nI find a reference I want to collect, and maybe even read. Delightful! I rub my little hands together gleefully and prepare to add it to Zotero. If its a new reference itâ€™ll generally have a DOI, which is the best of all possible worlds. I click the â€˜magic wandâ€™ icon in Zotero, paste in the DOI, and the reference appears in my collection.\n\n\n\n&lt;3 &lt;3 &lt;3\n\n\nIf a full-text PDF is freely available and scrapeable, Zotero finds and downloads it for me. Thereâ€™s a right-click &gt; â€˜Find available PDFâ€™ option that may need to be triggered. If that doesnâ€™t work, I do my best to find that fulltext myself7, and use Zotfileâ€™s context menu to add it as an attachment and automatically rename it and move it to âœ¨TheÂ Hoardâœ¨. I do my little extra tidying steps, and thatâ€™s that.\nFor older references, I add in the details and attach, move and rename files manually.\nOnce I want to start reading and taking notes, I move to Obsidian and use ctrl+shift+o to open the Citations dialog box, search and select the reference I want, and click to create a new note. The note appears, ready with basic metadata, places to add text and tags, and working links back to a) the Zotero entry itself and b) the source document. I can open the source document in Obsidian by using ctrl+click on the PDF link, and can pin it in the app alongside my notes page, letting me read and write easily.\n\n\n\n\nTake notes in Zotero? I used to, but it has very limited capability for cross-linking between notes, so no real capacity for generating syntheses. I like to do things like build literature timelines, and Obsidian is simply a better place for this. Zoteroâ€™s notes attachments are handy for a few things like meta notes I donâ€™t want to share (â€œthis paper sucks!â€) or PDF TOC exports, but otherwise I donâ€™t find them useful.\nUse Zoteroâ€™s â€˜Attach Stored Copyâ€™ functionality? Because then I canâ€™t easily get at the document except by using Zotero, and often I just want to find a source quickly and e.g.Â email it to someone. Stored copies on Windows wind up in \\\\Users\\%USERNAME%\\Zotero\\storage\\%RANDOMFOLDER%, where %RANDOMFOLDER% is an alphanumeric internally associated with the reference entry, making them poorly discoverable.\nAnnotate PDFs and export? Basically I donâ€™t remember anything I highlight, thatâ€™s just pretty colours. I have to rewrite things in my own words to take them in properly. I also work with a lot of old, bad PDF scans with crappy OCR or none at all, so it can be frustrating to even try. People who lean on a PDF-annotation workflow seem to never have to access anything older than 5-10 years, and that makes me equal parts jealous and suspicious.\n\n\n\n\n\nIâ€™m actually using this, instead of setting it up and then ignoring it!\nIâ€™m actually using this, instead of setting it up and then endlessly trying to optimise it!\nIf I donâ€™t use this for a few weeks, its not impossible to remember how everything works together.\nWithin notes I can add additional links to specific pages in the source pdf, using syntax like [[Darwin_1859_origin-Species-1e.pdf#page=112|Chapter 4 - Natural Selection]]. This is fabulous.\nI can also do (as needed) a bunch of other stuff like add stored images, paste in screenshots, attach/link to other files and other notes, embed maps, and add other kinds of rich content like youtube videos to my notes.\nI can link all this stuff to my â€˜daily notesâ€™ diary setup, also in Obsidian, and do some productivity/focus tracking and task management - a topic for another time8.\n\n\n\n\n\nOneDrive has to be running and logged in properly before interacting with the other software, otherwise things wonâ€™t sync correctly.\nI have to be careful to update Obsidian, Zotero, and the necessary plugins on both my home and work machines. I also need to close Obsidian before moving from one physical device to another to prevent sync issues.\nMy use of the â€˜Short Titleâ€™ field in Zotero is decidedly off-label and may cause problems in the long term. I also have to populate it manually before starting a new note, so adding a new reference takes longer.\nRe-addressing all the linked file attachments after moving âœ¨TheÂ Hoardâœ¨ seems to be a high-risk operation; somehow I didnâ€™t quite get it right the first time I moved it inside my Vault and have been slowly repairing links in Zotero ever since ğŸ˜• At least my naming convention makes things easy.\nObsidianâ€™s table support sucks, because tables suck when implemented in any plaintext format. I need and love tables. I donâ€™t see an immediate solution for this. Iâ€™d at least be happy if I could reliably copy+paste markdown table data between different apps, but thatâ€™s not a thing right now.\nZotero is annoying in some ways. Despite being definitely overall best in show for referencing:\n\nit lacks specific reference types for some things that really should be handled better, like datasets, databases, and preprints\nIt only enables the DOI field on a few item types, but people will whack a DOI on anything these days. I donâ€™t like having to store it in the â€˜extraâ€™ field!\nThe Windows GUI has a number of non-critical but highly irritating bugs, like laggy tabbing between fields.\nMass editing capability isnâ€™t quite there yet. Iâ€™d like to do some clean-up and consistency work, but thatâ€™s not easy right now.\nThe help forum has that clueless-newb vs hostile-oldtimer vibe so common to software and programming support. Its pretty mild compared to a lot of places, but still tiresome.\n\nThe â€˜linked filesâ€™ approach is good for me but maybe not optimal for sharing with colleaguesâ€¦but I can still potentially extend read access to âœ¨TheÂ Hoardâœ¨over OneDrive and they can import the *.bib file as needed - thereâ€™s just no auto-sync. I havenâ€™t tried this yet - there would certainly be some issues around integrating with their existing reference library that I donâ€™t want to have to debug.\nExporting notes in a normie-friendly format is a problem. Most of my colleagues donâ€™t use Obsidian or markdown. Iâ€™ve said it before, but some kind of Quarto-based Obsidian-alike would be amazing, then I could throw html files or PDFs at them all day.\n\nAnyway, thatâ€™s that on that."
  },
  {
    "objectID": "posts/2023-03-28_obsidian-setup-2023/index.html#background",
    "href": "posts/2023-03-28_obsidian-setup-2023/index.html#background",
    "title": "Research Notes Setup 2023",
    "section": "",
    "text": "Iâ€™ve seen a few posts and videos lately1 about how various people manage the avalanche of technical literature coming their way2. There are many helpful tips available in those posts, but nothing quite fit my own situation and needs end-to-end. So: this post is about how I currently manage a natural resource science-heavy reference library and associated knowledge generation infrastructure.\nTo be clear, I donâ€™t necessarily love this setup. Its the best Iâ€™ve managed so far though, so hopefully writing about what works and what could be better will be inspirational.\n\n\nI am:\n\na translational research worker with high anxiety and a short attention span\n\nI need/want:\n\nA setup that is compatible with my work IT environment but not dependent on it, because that is safe\nA setup that is relatively easy to upgrade and extend and generally futz around with, because that is fun.\n\n\n\n\nI am currently using:\n\nReference manager: Zotero, with the following extensions (â€˜add-onsâ€™):\n\nZotfile, for reference and source document management. Offers a one-click solution for moving a file into a designated spot and renaming it according to a given pattern.\nBetter Bibtex, which assigns unique IDs to Zotero references and automates their export to a *.bib file that can be picked up by other software that doesnâ€™t have close integration with Zotero.\n\nNotes manager: Obsidian, with the following extensions (â€˜community pluginsâ€™):\n\nCitations: picks up the *.bib file that Better Bibtex puts down, and allows me to generate a markdown-format literature note for each reference with a matching name, some nice metadata, and a pre-formatted space all ready for me to add my notes.\nAdmonition: (optional extra) makes it easy to add relative dates by e.g.Â typing in, e.g.Â @today, which then becomes a link named after todayâ€™s date.\nProjects: (optional extra) This plugin is very new, so maybe not super stable in the medium term, but I really like the ability to create table, kanban, gallery and calendar views of my notes. Its currently about as Notion or Anytype as you can get with markdown powering everything.\n\nStorage: Office 365 OneDrive. Its whatever, I get it for free through work, just about any cloud storage environment could be dropped in its place.\nAuthoring: Sometimes Obsidian, sometimes Quarto in RStudio, sometimes even (gasp) MS Word. The important part is that only Word and RStudio have full Zotero integration; other apps have to interact with it via the aforementioned *.bib file.\n\n\n\nIâ€™ve installed Zotero the usual way on a Windows machine. I have then added Zotfile, and set up my preferences as follows:\n\nGeneral Settings &gt; Location of Files: I have chosen â€˜Custom Locationâ€™, and instructed it to place any new linked file attachments in a particular folder in my OneDrive. I will call it âœ¨TheÂ Hoardâœ¨ going forward. The only important thing about âœ¨TheÂ Hoardâœ¨ is that it lives inside an Obsidian Vault. A Vault is just another folder, with some extra bits inside.\nRenaming Rules: This determines what the names of linked file attachments look like once theyâ€™re moved into âœ¨TheÂ Hoardâœ¨. I use the template {%a}_{%y}_{%t} , which is first author surname, year, title, separated by underscores. I have also ticked â€œchange to lower caseâ€, â€œtruncate title after . or : or ?â€3 , set â€œmaximum length of titleâ€ to 160, and maximum number of authors to 1. As such most of my linked files start out looking like, e.g.Â â€œdarwin_1859_on_the_origin_of_species.pdfâ€.\n\nI canâ€™t leave well enough alone, so I tend to do a bit more manual tweaking ğŸ™„. I add an â€œitem typeâ€ slug like _PAPER_ or _THESIS_ , I remove filler words like â€˜andâ€™, â€˜theâ€™ and â€˜novelâ€™ from the title, and I change underscores in titles to dashes, after Jenny Bryanâ€™s advice. They come out looking a bit like, e.g.Â â€œDarwin_1859_BOOK_origin-species-1e.pdfâ€. I do this because I want âœ¨TheÂ Hoardâœ¨ to be navigable on its own.\n\n\n\n\n\n\n\n\nTip\n\n\n\nMaximum length of title becomes important if you use cloud storage. OneDriveâ€™s maximum path length is 400 characters and some client software makes a fuss past 260 characters, so if your Hoard folder is nested deeply you have less to work with.\n\n\n\nAdvanced Settings: I expanded the â€˜Only work with the following filetypesâ€™ option to cover some extra attachment file types I often use, e.g.Â MS Office formats, common image formats, ebooks, markdown, html and zip files. If you donâ€™t do this, automated move-and-rename wonâ€™t work properly.\n\n\n\n\n\n\n\nNote\n\n\n\nThis option seems to mostly exist to protect against unintended consequences from enabling another setting in General Options - â€˜source folder for attaching new filesâ€™ grabs the most recently modified file in a nominated folder. This seems like more trouble than its worth, so if you actually use it, please explainâ€¦\n\n\nContinuing, I have added Better Bibtex, and left all the defaults except:\n\nAutomatic export: â€œWhen Idleâ€, delayed for 60 seconds. This is a compromise; I work with a lot of grey literature and other Old Stuff that has to be manually entered into Zotero. The default setting was fast enough that I was running into freeze-ups and sync errors because I was still editing a reference while BB was trying to re-export my whole library. Waiting longer means I sometimes have to manually trigger an export to make a new reference available outside Zotero.\nThe plugin is instructed to export my library to a *.bib file in Better Bibtex format that sits in the same folder as âœ¨TheÂ Hoardâœ¨.\n\n\n\n\nThese arenâ€™t important, but they are nice.\n\nZutilo - allows mass editing of reference item tags and attachment paths\nScite - scite.ai integration - citation records with context! I like what theyâ€™re doing. The service is still a bit incomplete in my main research fields, but improving all the time. One day Iâ€™ll convince my work to pay for itâ€¦\nWord integration - eh, why wouldnâ€™t you?\nMdnotes - not actually using this at the moment, but if you have a lot of notes already in Zotero and you want to move them into Obsidian, it would be very helpful.\n\n\n\n\nI have installed the software and set up a vault in my OneDrive called, imaginatively, Work_Stuff.\n\n\n\n\n\n\nImportant\n\n\n\nYou may have to pay for a commercial license to use Obsidian. Iâ€™m not currently as I donâ€™t qualify according to the EULA (NZ Crown Research Institutes are a weird public/private monstrosity, but fundamentally Iâ€™m employed as a non-profit government researcher). You may or may not have to depending on your circumstances, look into it and do the right thing.\n\n\nâœ¨TheÂ Hoardâœ¨ lives in a subfolder of Work_Stuff, as mentioned.\nI have then installed the Citations community plugin, with the following settings:\n\nCitation database format: BibLatex\nCitation database path: points at the *.bib file exported from Zotero by BB.\nLiterature Note folder: do what you want here. I started a specific Notes - sources folder inside the Vault (at the same level as âœ¨TheÂ Hoardâœ¨). You can get more specific and organise by project, or just store your notes in a big pile in the vaultâ€™s root directory if youâ€™re a filthy zoomer who doesnâ€™t understand file systems, Iâ€™m not judging4.\n\n\n\n\nMy templating choices are a little off-label, and this is where things get brittle. The key is that I use the â€˜Short Titleâ€™ field in Zotero to store the name of the file attachment I want to write about, sans file extension5. This means I can do two things: generate a note with a title that matches its source, and link to the source within the note template. In the main Zotero panel, an entry for Darwin (1859) might look like\n\n\n\n\n\nand in the item details:\n\n\n\n\n\nNow, in the Citations plugin settings,\n\nLiterature note title template: {titleShort}\nLiterature note content template:\n\ntitle: \"{{title}}\"\nfirst author: {{entry.data.creators.author.[0].lastName}}, {{entry.data.creators.author.[0].firstName}}\nyear: {{year}}\ndoi: {{DOI}}\ntype: {{entry.type}}\n---\nref::[Open reference in Zotero]({{zoteroSelectURI}})\npdf::[[{{titleShort}}.pdf|Open stored document]]\n***\n\n***\n## Abstract\n{{abstract}}\n\n## Notes\n\n## Cites\n\n## Cited by\n\n## Why I read this\n\n\n\n\n\n\n\nImportant\n\n\n\nThe double quotes around {title} stop the metadata breaking if you have any YAML-reserved characters in the article title, like :.\n\n\n\n\n\n\n\n\nNote\n\n\n\nI put tags in between the two *** rows, which render as a horizontal line in reading view.\n\n\nExample:\n\n\n\nI might even give this reference the attention it deserves at some pointâ€¦\n\n\nYouâ€™ll note the â€˜refâ€™ and â€˜pdfâ€™ rows use some syntax from the Obsidian Dataview plugin. This is a bit of future-proofing in case I get into using that extension - no immediate plans.\nI didnâ€™t originally have a â€˜why I read thisâ€™ line in the template, but Iâ€™ve found that useful for adding context to my notes. A lot of my sources have multiple potential applications so its become worth noting what I cared about each time I accessed them. This also helps me avoid falling down a rabbit-hole of chasing interesting facts when I donâ€™t have time - I can add a â€˜come back to thisâ€™ tag and just let it go otherwise, no pressure.\n\n\n\nI set up a Project over my Notes - sources folder, with a Table view. I pull in a few metadata fields that make finding things easy - note filename, first author, title, article type, year. At present thereâ€™s no proper date handling so year just looks like a large number, but it still works for sort/filter. An extra nice thing about Projects is that it forces me to make sure my note metadata is actually correctly populated and formatted6.\n\n\n\nNow you know what Iâ€™ve been reading lately ğŸ˜\n\n\n\n\n\n\nI find a reference I want to collect, and maybe even read. Delightful! I rub my little hands together gleefully and prepare to add it to Zotero. If its a new reference itâ€™ll generally have a DOI, which is the best of all possible worlds. I click the â€˜magic wandâ€™ icon in Zotero, paste in the DOI, and the reference appears in my collection.\n\n\n\n&lt;3 &lt;3 &lt;3\n\n\nIf a full-text PDF is freely available and scrapeable, Zotero finds and downloads it for me. Thereâ€™s a right-click &gt; â€˜Find available PDFâ€™ option that may need to be triggered. If that doesnâ€™t work, I do my best to find that fulltext myself7, and use Zotfileâ€™s context menu to add it as an attachment and automatically rename it and move it to âœ¨TheÂ Hoardâœ¨. I do my little extra tidying steps, and thatâ€™s that.\nFor older references, I add in the details and attach, move and rename files manually.\nOnce I want to start reading and taking notes, I move to Obsidian and use ctrl+shift+o to open the Citations dialog box, search and select the reference I want, and click to create a new note. The note appears, ready with basic metadata, places to add text and tags, and working links back to a) the Zotero entry itself and b) the source document. I can open the source document in Obsidian by using ctrl+click on the PDF link, and can pin it in the app alongside my notes page, letting me read and write easily.\n\n\n\n\nTake notes in Zotero? I used to, but it has very limited capability for cross-linking between notes, so no real capacity for generating syntheses. I like to do things like build literature timelines, and Obsidian is simply a better place for this. Zoteroâ€™s notes attachments are handy for a few things like meta notes I donâ€™t want to share (â€œthis paper sucks!â€) or PDF TOC exports, but otherwise I donâ€™t find them useful.\nUse Zoteroâ€™s â€˜Attach Stored Copyâ€™ functionality? Because then I canâ€™t easily get at the document except by using Zotero, and often I just want to find a source quickly and e.g.Â email it to someone. Stored copies on Windows wind up in \\\\Users\\%USERNAME%\\Zotero\\storage\\%RANDOMFOLDER%, where %RANDOMFOLDER% is an alphanumeric internally associated with the reference entry, making them poorly discoverable.\nAnnotate PDFs and export? Basically I donâ€™t remember anything I highlight, thatâ€™s just pretty colours. I have to rewrite things in my own words to take them in properly. I also work with a lot of old, bad PDF scans with crappy OCR or none at all, so it can be frustrating to even try. People who lean on a PDF-annotation workflow seem to never have to access anything older than 5-10 years, and that makes me equal parts jealous and suspicious.\n\n\n\n\n\nIâ€™m actually using this, instead of setting it up and then ignoring it!\nIâ€™m actually using this, instead of setting it up and then endlessly trying to optimise it!\nIf I donâ€™t use this for a few weeks, its not impossible to remember how everything works together.\nWithin notes I can add additional links to specific pages in the source pdf, using syntax like [[Darwin_1859_origin-Species-1e.pdf#page=112|Chapter 4 - Natural Selection]]. This is fabulous.\nI can also do (as needed) a bunch of other stuff like add stored images, paste in screenshots, attach/link to other files and other notes, embed maps, and add other kinds of rich content like youtube videos to my notes.\nI can link all this stuff to my â€˜daily notesâ€™ diary setup, also in Obsidian, and do some productivity/focus tracking and task management - a topic for another time8.\n\n\n\n\n\nOneDrive has to be running and logged in properly before interacting with the other software, otherwise things wonâ€™t sync correctly.\nI have to be careful to update Obsidian, Zotero, and the necessary plugins on both my home and work machines. I also need to close Obsidian before moving from one physical device to another to prevent sync issues.\nMy use of the â€˜Short Titleâ€™ field in Zotero is decidedly off-label and may cause problems in the long term. I also have to populate it manually before starting a new note, so adding a new reference takes longer.\nRe-addressing all the linked file attachments after moving âœ¨TheÂ Hoardâœ¨ seems to be a high-risk operation; somehow I didnâ€™t quite get it right the first time I moved it inside my Vault and have been slowly repairing links in Zotero ever since ğŸ˜• At least my naming convention makes things easy.\nObsidianâ€™s table support sucks, because tables suck when implemented in any plaintext format. I need and love tables. I donâ€™t see an immediate solution for this. Iâ€™d at least be happy if I could reliably copy+paste markdown table data between different apps, but thatâ€™s not a thing right now.\nZotero is annoying in some ways. Despite being definitely overall best in show for referencing:\n\nit lacks specific reference types for some things that really should be handled better, like datasets, databases, and preprints\nIt only enables the DOI field on a few item types, but people will whack a DOI on anything these days. I donâ€™t like having to store it in the â€˜extraâ€™ field!\nThe Windows GUI has a number of non-critical but highly irritating bugs, like laggy tabbing between fields.\nMass editing capability isnâ€™t quite there yet. Iâ€™d like to do some clean-up and consistency work, but thatâ€™s not easy right now.\nThe help forum has that clueless-newb vs hostile-oldtimer vibe so common to software and programming support. Its pretty mild compared to a lot of places, but still tiresome.\n\nThe â€˜linked filesâ€™ approach is good for me but maybe not optimal for sharing with colleaguesâ€¦but I can still potentially extend read access to âœ¨TheÂ Hoardâœ¨over OneDrive and they can import the *.bib file as needed - thereâ€™s just no auto-sync. I havenâ€™t tried this yet - there would certainly be some issues around integrating with their existing reference library that I donâ€™t want to have to debug.\nExporting notes in a normie-friendly format is a problem. Most of my colleagues donâ€™t use Obsidian or markdown. Iâ€™ve said it before, but some kind of Quarto-based Obsidian-alike would be amazing, then I could throw html files or PDFs at them all day.\n\nAnyway, thatâ€™s that on that."
  },
  {
    "objectID": "posts/2023-03-28_obsidian-setup-2023/index.html#footnotes",
    "href": "posts/2023-03-28_obsidian-setup-2023/index.html#footnotes",
    "title": "Research Notes Setup 2023",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsearch â€˜obsidian zoteroâ€™, youâ€™ll be occupied for a whileâ†©ï¸\nLike, technically, not emotionally.â†©ï¸\nNothing good ever happens after that point.â†©ï¸\nYes, I am.â†©ï¸\nIt doesnâ€™t seem to be used for anything else, near as I can tell, but thatâ€™s not future-proof.â†©ï¸\nThe trick going forward will be to not accidentally rebuild Zotero in Obsidian.â†©ï¸\nIYKNKâ†©ï¸\nNo promises.â†©ï¸"
  },
  {
    "objectID": "posts/2018-12-31_slga-announcement/index.html",
    "href": "posts/2018-12-31_slga-announcement/index.html",
    "title": "New R package: slga",
    "section": "",
    "text": "Catching up on package blogging, and juuuust managing to equal the low, low bar of four posts per year that I appear to have set myself.\nThis post is about slga, a data-access package I wrote a month or so ago to facilitate R-based access to the Soil and Landscape Grid of Australia, a set of geostatistically-modelled soil attributes and accompanying environmental covariate datasets.\nslga is another one of those packages that happened because I read some interesting code (in this case, Ross Searleâ€™s WCS access demo script) and decided to tinker a bit and thenâ€¦ failed to stop. Whoops. The basic idea is to hook into the set of OGC Web Coverage Services available for the SLGA and make it as easy as possible to retrieve subsets of the parent datasets. My only requirement was that the subsets be â€˜cleanâ€™; i.e.Â a perfect match in terms of cell value, cell size and alignment against the parent dataset. And thus the following:\n\nlibrary(raster)\nlibrary(slga)\nlibrary(ggplot2)\n\n# get surface clay content for King Island\naoi &lt;- c(143.75, -40.17, 144.18, -39.57)\nki_surface_clay &lt;- get_slga_data(product = 'TAS', attribute = 'CLY',\n                                 component = 'all', depth = 1,\n                                 aoi = aoi, write_out = FALSE)\n\nretrieves this:\n\n\n\n\n\nWCS services arenâ€™t quite designed for this task - theyâ€™re mainly geared towards dynamic data access via web map portals or GIS GUIs, so they default to a lot of dynamic rescaling and resampling to make that efficient. Still, with a bit of mucking about its possible to bend them towards simple subsetting of very large raster datasets, at pretty reasonable speed (depending, of course, on oneâ€™s internet connection, insert NBN joke here).\nMy other goal for this project was to figure out pkgdown, so Iâ€™m not going to reiterate how slga works in this post. Iâ€™m just going to smugly link to slga's vignette where it sits on the package website.\n\n\nI really didnâ€™t get OGC web services at all before diving in to this. The official documentation is pretty comprehensive, but I couldnâ€™t find much higher level material about working with them. I definitely wouldnâ€™t have known where to start without picking through Rossâ€™ script; slga only exists because of his work (as does the grid itself!).\nI found Lorenzo Busettoâ€™s tutorial post immensely helpful when getting started with pkgdown. Datacampâ€™s deployment tutorial was also super good. For customising my site, I left the default bootstrap theme in place and just overrode some CSS for nicer colours. Lest anyone think Iâ€™m actually good at CSS, this was largely accomplished by clicking â€˜Inspect Elementâ€™ in Firefox and copypasting the relevant CSS code out. It is therefore probable that my extra.css file is an affront to god and man alike, but whatever, it works.\nLastly, this trick for mosaicing a list of rasters is the beesâ€™ knees, and enabled tiling requests over larger areas. That said, I still wouldnâ€™t advise trying to download massive data extents at once with this package. Once you start going after whole stateâ€™s worth of data, youâ€™re better off downloading the entire parent dataset from the CSIRO Data Access Portal and cropping it.\n\n\n\nThe only thing I donâ€™t love about this project is the low-ish unit test coverage. Iâ€™m not sure how best to cover some of the core functions, since they hit web services, and the tests shouldnâ€™t be eating bandwidth or relying on a http 200 return. If anyone has any advice, fire away.\nI also really wish the WCS spec had some kind of source-align/target-align flag a la gdalwarpâ€™s â€˜-tapâ€™ option, because that would remove the need for about half the code I wrote for this package. Might stretching the concept too far though, idk.\n\nAnyway so there that is, if youâ€™re working in Aus and you need a bit of quick soil and/or terrain data, this may be useful. Iâ€™m also drafting up a matching package for GeoScience Australiaâ€™s web services, so watch this space."
  },
  {
    "objectID": "posts/2018-12-31_slga-announcement/index.html#background",
    "href": "posts/2018-12-31_slga-announcement/index.html#background",
    "title": "New R package: slga",
    "section": "",
    "text": "Catching up on package blogging, and juuuust managing to equal the low, low bar of four posts per year that I appear to have set myself.\nThis post is about slga, a data-access package I wrote a month or so ago to facilitate R-based access to the Soil and Landscape Grid of Australia, a set of geostatistically-modelled soil attributes and accompanying environmental covariate datasets.\nslga is another one of those packages that happened because I read some interesting code (in this case, Ross Searleâ€™s WCS access demo script) and decided to tinker a bit and thenâ€¦ failed to stop. Whoops. The basic idea is to hook into the set of OGC Web Coverage Services available for the SLGA and make it as easy as possible to retrieve subsets of the parent datasets. My only requirement was that the subsets be â€˜cleanâ€™; i.e.Â a perfect match in terms of cell value, cell size and alignment against the parent dataset. And thus the following:\n\nlibrary(raster)\nlibrary(slga)\nlibrary(ggplot2)\n\n# get surface clay content for King Island\naoi &lt;- c(143.75, -40.17, 144.18, -39.57)\nki_surface_clay &lt;- get_slga_data(product = 'TAS', attribute = 'CLY',\n                                 component = 'all', depth = 1,\n                                 aoi = aoi, write_out = FALSE)\n\nretrieves this:\n\n\n\n\n\nWCS services arenâ€™t quite designed for this task - theyâ€™re mainly geared towards dynamic data access via web map portals or GIS GUIs, so they default to a lot of dynamic rescaling and resampling to make that efficient. Still, with a bit of mucking about its possible to bend them towards simple subsetting of very large raster datasets, at pretty reasonable speed (depending, of course, on oneâ€™s internet connection, insert NBN joke here).\nMy other goal for this project was to figure out pkgdown, so Iâ€™m not going to reiterate how slga works in this post. Iâ€™m just going to smugly link to slga's vignette where it sits on the package website.\n\n\nI really didnâ€™t get OGC web services at all before diving in to this. The official documentation is pretty comprehensive, but I couldnâ€™t find much higher level material about working with them. I definitely wouldnâ€™t have known where to start without picking through Rossâ€™ script; slga only exists because of his work (as does the grid itself!).\nI found Lorenzo Busettoâ€™s tutorial post immensely helpful when getting started with pkgdown. Datacampâ€™s deployment tutorial was also super good. For customising my site, I left the default bootstrap theme in place and just overrode some CSS for nicer colours. Lest anyone think Iâ€™m actually good at CSS, this was largely accomplished by clicking â€˜Inspect Elementâ€™ in Firefox and copypasting the relevant CSS code out. It is therefore probable that my extra.css file is an affront to god and man alike, but whatever, it works.\nLastly, this trick for mosaicing a list of rasters is the beesâ€™ knees, and enabled tiling requests over larger areas. That said, I still wouldnâ€™t advise trying to download massive data extents at once with this package. Once you start going after whole stateâ€™s worth of data, youâ€™re better off downloading the entire parent dataset from the CSIRO Data Access Portal and cropping it.\n\n\n\nThe only thing I donâ€™t love about this project is the low-ish unit test coverage. Iâ€™m not sure how best to cover some of the core functions, since they hit web services, and the tests shouldnâ€™t be eating bandwidth or relying on a http 200 return. If anyone has any advice, fire away.\nI also really wish the WCS spec had some kind of source-align/target-align flag a la gdalwarpâ€™s â€˜-tapâ€™ option, because that would remove the need for about half the code I wrote for this package. Might stretching the concept too far though, idk.\n\nAnyway so there that is, if youâ€™re working in Aus and you need a bit of quick soil and/or terrain data, this may be useful. Iâ€™m also drafting up a matching package for GeoScience Australiaâ€™s web services, so watch this space."
  },
  {
    "objectID": "posts/2018-08-01_fitbit-api-r/index.html",
    "href": "posts/2018-08-01_fitbit-api-r/index.html",
    "title": "Playing with the Fitbit API in R",
    "section": "",
    "text": "So Iâ€™ve been going to this boxing/HIIT style gym for a while, which is fantastic because it turns out I love punching things and suffering. I almost have abs now! The gym is quite big on using personal analytics to track progress, so they offer the MyZone brand of fitness tracker, which is centred around the use of a chest strap. The strap monitors heart rate and uses it to calculate â€˜MyZone Effort Pointsâ€™ (MEPS). More suffering == more points, and thereâ€™s a big TV in the gym where MyZone userâ€™s scores are visible for comparison. Iâ€™m well aware that the only person Iâ€™m really competing with is me-last-week, but Iâ€™m still curious about what kind of scores I get during my workouts.\nIâ€™ve worn a Fitbit for several years, is the thing, and Iâ€™m both averse to change and an inveterate cheapskate, so I didnâ€™t really want to switch brands (or worse, wear both). So, how can I compare the data FitBit records for me when Iâ€™m working out to the data recorded by MyZone?\nR, of course. And the magic of APIs. Read on for how.\nFirst, I needed to see if I could access minute-by-minute heart rate data from FitBit, because I couldnâ€™t calculate MEPS without that. Iâ€™m using a Charge 2 at present, and the FitBit mobile app provides graphs of heart-rate data during activities that are clearly high-resolution. That dataâ€™s gotta be somewhere and I wants it *grabby hands*\nThere are a few blog posts around about accessing FitBit data in R, but they seem to date quickly - access methods have not been very stable over time. Same goes for Stack Overflow and Fitbitâ€™s own forums! There was also a package or two floating about, but they appear defunct. Distilling a few posts like this one and this one together with the official documentation got me where I needed to be, so current to July 2018, my procedure to get at my own intraday heart rate data is this:\n\nGo to https://dev.fitbit.com/ and click on Manage &gt; Register an App.\nLogin with fitbit credentials and follow the prompts. Name the â€˜appâ€™ something neutral like â€˜my-dataâ€™, choose app type â€˜Personalâ€™ and throw in www.google.com or something similar wherever a URL is asked for (the only important one is callback url).\n\n\n\n\n\n\n\nOnce the app is created, click on the â€˜OAuth 2.0 tutorial pageâ€™ link near the bottom of the screen:\n\n\n\n\n\n\n\nScroll down to the end of section 1 and click on the provided authorisation URL. A new tab will open, showing the FitBit authorisation interface:\n\n\n\n\n\n\n\nI changed the auth period to 1 year so I donâ€™t have to go through this too often, and ticked all options before clicking Allow, because Iâ€™ll probably play with the other endpoints at some point.\nAfter clicking Allow, the browser tab redirects to the callback URL, but a whole lot of other stuff is now in the URL visible in the address bar. Copy the whole lot and go back to the OAuth 2.0 tutorial page. Paste that URL into the text box under the â€˜2. Parse Responseâ€™ header. The access token will appear below the text box - its a long string of characters.\nSave that token as an environment variable so its not stored unsecured. Clicking around on Windows, this is Control Panel &gt; System &gt; Advanced System Settings &gt; Environment Variablesâ€¦ . I save mine as a User variable called â€˜FITB_AUTHâ€™. NB: Youâ€™ll then have to close and reopen RStudio if you do all this while its running.\n\nWith all that in place, I can finally write some damn R code.\n\nlibrary(httr)\nlibrary(tidyverse)\n\nBefore starting to extract data, its a good idea to check that oneâ€™s token is definitely working. The FitBit API has a â€œRetrieve State of Tokensâ€ endpoint for this:\n\ncheck_state &lt;- function(token = NULL) {\n  POST(url = 'https://api.fitbit.com/1.1/oauth2/introspect',\n       add_headers(Authorization = paste0('Bearer ', token)),\n       body = paste0('token=', token),\n       content_type('application/x-www-form-urlencoded'))\n}\n\nstate &lt;- check_state(token = Sys.getenv('FITB_AUTH'))\n\ncontent(state)$active\n\n## [1] TRUE\nMy token is active, so I can proceed.\nMy last workout was Monday night. I used the Charge 2â€™s manual logging feature, so I know exactly when my workout started and finished. A GET request to retrieve minute-by-minute heart rate data for a set period looks like:\n\nget_workout &lt;- function(date = NULL, start_time = NULL, end_time = NULL, \n                        token = Sys.getenv('FITB_AUTH')) {\n  GET(url =\n        paste0('https://api.fitbit.com/1/user/-/activities/heart/date/',\n               date, '/1d/1min/time/', start_time, '/', end_time, '.json'),\n      add_headers(Authorization = paste0(\"Bearer \", token)))\n}\n\ngot_workout &lt;- get_workout(date = '2018-07-30', \n                           start_time = '18:47', end_time = '19:28')\n\nThe actual data can be seen using content():\n\nworkout &lt;- content(got_workout)\n\nIt does come in all list-ified, which can be a little tricky to sort out. purrr functions are real lifesavers here. Iâ€™m going to turn a couple of sub-lists into data frames below - FitBitâ€™s own workout summary, and the actual heart rate by minute data Iâ€™m after:\n\n# json-as-list to dataframe (for simple cases without nesting!)\njsonlist_to_df &lt;- function(data = NULL) {\n    purrr::transpose(data) %&gt;%\n    purrr::map(., unlist) %&gt;%\n    as_tibble(., stringsAsFactors = FALSE)\n}\n\n# summary\nworkout[['activities-heart']][[1]][['heartRateZones']] &lt;- \n  jsonlist_to_df(workout[['activities-heart']][[1]][['heartRateZones']])\n\n# the good stuff\nworkout[['activities-heart-intraday']][['dataset']] &lt;-\n  jsonlist_to_df(workout[['activities-heart-intraday']][['dataset']]) \n\n# also let's get time formatted properly\nworkout$`activities-heart-intraday`$dataset$time &lt;- \n  as.POSIXlt(workout$`activities-heart-intraday`$dataset$time, format = '%H:%M:%S')\nlubridate::date(workout$`activities-heart-intraday`$dataset$time) &lt;- '2018-07-30'\nlubridate::tz(workout$`activities-heart-intraday`$dataset$time) &lt;- 'Australia/Brisbane'\n\n# looks better now:\nworkout\n\n## $`activities-heart`\n## $`activities-heart`[[1]]\n## $`activities-heart`[[1]]$customHeartRateZones\n## list()\n## \n## $`activities-heart`[[1]]$dateTime\n## [1] \"2018-07-30\"\n## \n## $`activities-heart`[[1]]$heartRateZones\n## # A tibble: 4 x 5\n##   caloriesOut   max   min minutes name        \n##         &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;chr&gt;       \n## 1        0       93    30       0 Out of Range\n## 2       62.1    130    93      11 Fat Burn    \n## 3      230.     158   130      30 Cardio      \n## 4        9.30   220   158       1 Peak        \n## \n## $`activities-heart`[[1]]$value\n## [1] \"136.67\"\n## \n## \n## \n## $`activities-heart-intraday`\n## $`activities-heart-intraday`$dataset\n## # A tibble: 42 x 2\n##    time                value\n##    &lt;S3: POSIXlt&gt;       &lt;int&gt;\n##  1 2018-07-30 18:47:00    99\n##  2 2018-07-30 18:48:00   113\n##  3 2018-07-30 18:49:00   135\n##  4 2018-07-30 18:50:00   146\n##  5 2018-07-30 18:51:00   153\n##  6 2018-07-30 18:52:00   152\n##  7 2018-07-30 18:53:00   155\n##  8 2018-07-30 18:54:00   160\n##  9 2018-07-30 18:55:00   152\n## 10 2018-07-30 18:56:00   151\n## # ... with 32 more rows\n## \n## $`activities-heart-intraday`$datasetInterval\n## [1] 1\n## \n## $`activities-heart-intraday`$datasetType\n## [1] \"minute\"\nNow, as I mentioned way back up top, MEPS are calculated minute-by-minute as a percentage of max heart rate. The formula used to calculate max heart rate is on the MyZone website:\n\nmeps_max &lt;- function(age = NULL) { 207 - (0.7 * age) }\n\nWhich makes mine 183. Note that this can be less accurate for some people, but as a depressingly average soul I donâ€™t get to complain. Five MEPS zones are defined using percentage range of max HR (e.g.Â 3 MEPS/min at 70-79% of max HR), and I can calculate the appropriate heart-rate ranges for myself like this:\n\n# I &lt;3 tribble\nmy_MEPS &lt;- tribble(~MEPS, ~hr_range, ~hr_lo, ~hr_hi, \n                       1,  '50-59%',   0.50,   0.59,\n                       2,  '60-69%',   0.60,   0.69,\n                       3,  '70-79%',   0.70,   0.79,\n                       4,    '&gt;=80',   0.80,   1.00) %&gt;%\n  mutate(my_hr_low = floor(meps_max(34) * hr_lo),\n         my_hr_hi  = ceiling(meps_max(34) * hr_hi))\nmy_MEPS\n\n## # A tibble: 4 x 6\n##    MEPS hr_range hr_lo hr_hi my_hr_low my_hr_hi\n##   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1     1 50-59%     0.5  0.59        91      109\n## 2     2 60-69%     0.6  0.69       109      127\n## 3     3 70-79%     0.7  0.79       128      145\n## 4     4 &gt;=80       0.8  1          146      184\nUsing that data, calculating total MEPS goes like this:\n\nmutate(workout$`activities-heart-intraday`$dataset,\n       meps = case_when(value &gt;= 146 ~ 4,\n                        value &gt;= 128 ~ 3,\n                        value &gt;= 109 ~ 2,\n                        value &gt;= 91  ~ 1,\n                        TRUE ~ 0)) %&gt;%\n  summarise(\"Total MEPS\" = sum(meps))\n\n## # A tibble: 1 x 1\n##   `Total MEPS`\n##          &lt;dbl&gt;\n## 1          130\nGiven that the maximum possible MEPS in a 42-minute workout is 168, this isnâ€™t too bad. Its also fairly consistent with past workouts. I kind of knew Iâ€™d done ok from how I had trouble lifting my arms after, but its nice to have the numbers to back that up :P\n\nSo there it is. Unfortunately this setup still isnâ€™t ideal - httrâ€™s Oauth 2.0 authorisation code flow doesnâ€™t seem to quite work for FitBit and Iâ€™m not 100% sure why. The request functions above would be a bit simpler if I could get the â€˜oauth danceâ€™ to work, and I could also skip that whole â€˜faffing around on dev.fitbit.comâ€™ section. As it stands, I get to the oauth2.0_token() step and then it just sits in the browser while waiting for authentication. Iâ€™ve been poking away at this post on and off for literal months trying to crack it, but no dice. So whatever, perfect is the enemy of good and I know as soon as I post this, someoneâ€™s going to reply with an amazingly simple solution :P"
  },
  {
    "objectID": "posts/2018-08-01_fitbit-api-r/index.html#background",
    "href": "posts/2018-08-01_fitbit-api-r/index.html#background",
    "title": "Playing with the Fitbit API in R",
    "section": "",
    "text": "So Iâ€™ve been going to this boxing/HIIT style gym for a while, which is fantastic because it turns out I love punching things and suffering. I almost have abs now! The gym is quite big on using personal analytics to track progress, so they offer the MyZone brand of fitness tracker, which is centred around the use of a chest strap. The strap monitors heart rate and uses it to calculate â€˜MyZone Effort Pointsâ€™ (MEPS). More suffering == more points, and thereâ€™s a big TV in the gym where MyZone userâ€™s scores are visible for comparison. Iâ€™m well aware that the only person Iâ€™m really competing with is me-last-week, but Iâ€™m still curious about what kind of scores I get during my workouts.\nIâ€™ve worn a Fitbit for several years, is the thing, and Iâ€™m both averse to change and an inveterate cheapskate, so I didnâ€™t really want to switch brands (or worse, wear both). So, how can I compare the data FitBit records for me when Iâ€™m working out to the data recorded by MyZone?\nR, of course. And the magic of APIs. Read on for how.\nFirst, I needed to see if I could access minute-by-minute heart rate data from FitBit, because I couldnâ€™t calculate MEPS without that. Iâ€™m using a Charge 2 at present, and the FitBit mobile app provides graphs of heart-rate data during activities that are clearly high-resolution. That dataâ€™s gotta be somewhere and I wants it *grabby hands*\nThere are a few blog posts around about accessing FitBit data in R, but they seem to date quickly - access methods have not been very stable over time. Same goes for Stack Overflow and Fitbitâ€™s own forums! There was also a package or two floating about, but they appear defunct. Distilling a few posts like this one and this one together with the official documentation got me where I needed to be, so current to July 2018, my procedure to get at my own intraday heart rate data is this:\n\nGo to https://dev.fitbit.com/ and click on Manage &gt; Register an App.\nLogin with fitbit credentials and follow the prompts. Name the â€˜appâ€™ something neutral like â€˜my-dataâ€™, choose app type â€˜Personalâ€™ and throw in www.google.com or something similar wherever a URL is asked for (the only important one is callback url).\n\n\n\n\n\n\n\nOnce the app is created, click on the â€˜OAuth 2.0 tutorial pageâ€™ link near the bottom of the screen:\n\n\n\n\n\n\n\nScroll down to the end of section 1 and click on the provided authorisation URL. A new tab will open, showing the FitBit authorisation interface:\n\n\n\n\n\n\n\nI changed the auth period to 1 year so I donâ€™t have to go through this too often, and ticked all options before clicking Allow, because Iâ€™ll probably play with the other endpoints at some point.\nAfter clicking Allow, the browser tab redirects to the callback URL, but a whole lot of other stuff is now in the URL visible in the address bar. Copy the whole lot and go back to the OAuth 2.0 tutorial page. Paste that URL into the text box under the â€˜2. Parse Responseâ€™ header. The access token will appear below the text box - its a long string of characters.\nSave that token as an environment variable so its not stored unsecured. Clicking around on Windows, this is Control Panel &gt; System &gt; Advanced System Settings &gt; Environment Variablesâ€¦ . I save mine as a User variable called â€˜FITB_AUTHâ€™. NB: Youâ€™ll then have to close and reopen RStudio if you do all this while its running.\n\nWith all that in place, I can finally write some damn R code.\n\nlibrary(httr)\nlibrary(tidyverse)\n\nBefore starting to extract data, its a good idea to check that oneâ€™s token is definitely working. The FitBit API has a â€œRetrieve State of Tokensâ€ endpoint for this:\n\ncheck_state &lt;- function(token = NULL) {\n  POST(url = 'https://api.fitbit.com/1.1/oauth2/introspect',\n       add_headers(Authorization = paste0('Bearer ', token)),\n       body = paste0('token=', token),\n       content_type('application/x-www-form-urlencoded'))\n}\n\nstate &lt;- check_state(token = Sys.getenv('FITB_AUTH'))\n\ncontent(state)$active\n\n## [1] TRUE\nMy token is active, so I can proceed.\nMy last workout was Monday night. I used the Charge 2â€™s manual logging feature, so I know exactly when my workout started and finished. A GET request to retrieve minute-by-minute heart rate data for a set period looks like:\n\nget_workout &lt;- function(date = NULL, start_time = NULL, end_time = NULL, \n                        token = Sys.getenv('FITB_AUTH')) {\n  GET(url =\n        paste0('https://api.fitbit.com/1/user/-/activities/heart/date/',\n               date, '/1d/1min/time/', start_time, '/', end_time, '.json'),\n      add_headers(Authorization = paste0(\"Bearer \", token)))\n}\n\ngot_workout &lt;- get_workout(date = '2018-07-30', \n                           start_time = '18:47', end_time = '19:28')\n\nThe actual data can be seen using content():\n\nworkout &lt;- content(got_workout)\n\nIt does come in all list-ified, which can be a little tricky to sort out. purrr functions are real lifesavers here. Iâ€™m going to turn a couple of sub-lists into data frames below - FitBitâ€™s own workout summary, and the actual heart rate by minute data Iâ€™m after:\n\n# json-as-list to dataframe (for simple cases without nesting!)\njsonlist_to_df &lt;- function(data = NULL) {\n    purrr::transpose(data) %&gt;%\n    purrr::map(., unlist) %&gt;%\n    as_tibble(., stringsAsFactors = FALSE)\n}\n\n# summary\nworkout[['activities-heart']][[1]][['heartRateZones']] &lt;- \n  jsonlist_to_df(workout[['activities-heart']][[1]][['heartRateZones']])\n\n# the good stuff\nworkout[['activities-heart-intraday']][['dataset']] &lt;-\n  jsonlist_to_df(workout[['activities-heart-intraday']][['dataset']]) \n\n# also let's get time formatted properly\nworkout$`activities-heart-intraday`$dataset$time &lt;- \n  as.POSIXlt(workout$`activities-heart-intraday`$dataset$time, format = '%H:%M:%S')\nlubridate::date(workout$`activities-heart-intraday`$dataset$time) &lt;- '2018-07-30'\nlubridate::tz(workout$`activities-heart-intraday`$dataset$time) &lt;- 'Australia/Brisbane'\n\n# looks better now:\nworkout\n\n## $`activities-heart`\n## $`activities-heart`[[1]]\n## $`activities-heart`[[1]]$customHeartRateZones\n## list()\n## \n## $`activities-heart`[[1]]$dateTime\n## [1] \"2018-07-30\"\n## \n## $`activities-heart`[[1]]$heartRateZones\n## # A tibble: 4 x 5\n##   caloriesOut   max   min minutes name        \n##         &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;chr&gt;       \n## 1        0       93    30       0 Out of Range\n## 2       62.1    130    93      11 Fat Burn    \n## 3      230.     158   130      30 Cardio      \n## 4        9.30   220   158       1 Peak        \n## \n## $`activities-heart`[[1]]$value\n## [1] \"136.67\"\n## \n## \n## \n## $`activities-heart-intraday`\n## $`activities-heart-intraday`$dataset\n## # A tibble: 42 x 2\n##    time                value\n##    &lt;S3: POSIXlt&gt;       &lt;int&gt;\n##  1 2018-07-30 18:47:00    99\n##  2 2018-07-30 18:48:00   113\n##  3 2018-07-30 18:49:00   135\n##  4 2018-07-30 18:50:00   146\n##  5 2018-07-30 18:51:00   153\n##  6 2018-07-30 18:52:00   152\n##  7 2018-07-30 18:53:00   155\n##  8 2018-07-30 18:54:00   160\n##  9 2018-07-30 18:55:00   152\n## 10 2018-07-30 18:56:00   151\n## # ... with 32 more rows\n## \n## $`activities-heart-intraday`$datasetInterval\n## [1] 1\n## \n## $`activities-heart-intraday`$datasetType\n## [1] \"minute\"\nNow, as I mentioned way back up top, MEPS are calculated minute-by-minute as a percentage of max heart rate. The formula used to calculate max heart rate is on the MyZone website:\n\nmeps_max &lt;- function(age = NULL) { 207 - (0.7 * age) }\n\nWhich makes mine 183. Note that this can be less accurate for some people, but as a depressingly average soul I donâ€™t get to complain. Five MEPS zones are defined using percentage range of max HR (e.g.Â 3 MEPS/min at 70-79% of max HR), and I can calculate the appropriate heart-rate ranges for myself like this:\n\n# I &lt;3 tribble\nmy_MEPS &lt;- tribble(~MEPS, ~hr_range, ~hr_lo, ~hr_hi, \n                       1,  '50-59%',   0.50,   0.59,\n                       2,  '60-69%',   0.60,   0.69,\n                       3,  '70-79%',   0.70,   0.79,\n                       4,    '&gt;=80',   0.80,   1.00) %&gt;%\n  mutate(my_hr_low = floor(meps_max(34) * hr_lo),\n         my_hr_hi  = ceiling(meps_max(34) * hr_hi))\nmy_MEPS\n\n## # A tibble: 4 x 6\n##    MEPS hr_range hr_lo hr_hi my_hr_low my_hr_hi\n##   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1     1 50-59%     0.5  0.59        91      109\n## 2     2 60-69%     0.6  0.69       109      127\n## 3     3 70-79%     0.7  0.79       128      145\n## 4     4 &gt;=80       0.8  1          146      184\nUsing that data, calculating total MEPS goes like this:\n\nmutate(workout$`activities-heart-intraday`$dataset,\n       meps = case_when(value &gt;= 146 ~ 4,\n                        value &gt;= 128 ~ 3,\n                        value &gt;= 109 ~ 2,\n                        value &gt;= 91  ~ 1,\n                        TRUE ~ 0)) %&gt;%\n  summarise(\"Total MEPS\" = sum(meps))\n\n## # A tibble: 1 x 1\n##   `Total MEPS`\n##          &lt;dbl&gt;\n## 1          130\nGiven that the maximum possible MEPS in a 42-minute workout is 168, this isnâ€™t too bad. Its also fairly consistent with past workouts. I kind of knew Iâ€™d done ok from how I had trouble lifting my arms after, but its nice to have the numbers to back that up :P\n\nSo there it is. Unfortunately this setup still isnâ€™t ideal - httrâ€™s Oauth 2.0 authorisation code flow doesnâ€™t seem to quite work for FitBit and Iâ€™m not 100% sure why. The request functions above would be a bit simpler if I could get the â€˜oauth danceâ€™ to work, and I could also skip that whole â€˜faffing around on dev.fitbit.comâ€™ section. As it stands, I get to the oauth2.0_token() step and then it just sits in the browser while waiting for authentication. Iâ€™ve been poking away at this post on and off for literal months trying to crack it, but no dice. So whatever, perfect is the enemy of good and I know as soon as I post this, someoneâ€™s going to reply with an amazingly simple solution :P"
  },
  {
    "objectID": "posts/2017-10-22_learning-shiny/index.html",
    "href": "posts/2017-10-22_learning-shiny/index.html",
    "title": "Learning Shiny with the Spline Tool",
    "section": "",
    "text": "My current project is about to produce a Giant Heap of data for end users to play with, and Iâ€™m concerned that it might be a bit overwhelming to digest. Even Iâ€™m having trouble trawling through it all to make sure everything is correct. A web app that allows the user to drill into that heap and just pull out what they need may be necessaryâ€¦better learn how to build one, I guess!\nIâ€™ve done just about everything else for the project in R, so I figured Iâ€™d maintain consistency and learn Shiny. As a bit of a â€˜Hello Worldâ€™ project, I decided to try and replicate a small standalone app used by soil scientists to pre-process soil laboratory data.\nSoil lab data is collected on a sample basis: you dig your hole, you grab ~200-500g of soil within a set of given depth ranges, you bag the samples up, and send them to the lab. Budget and time constraints generally mean that you donâ€™t get to sample every depth interval in a profile, so you must attempt to pick representative depth ranges. Best practice is one sample per horizon and/or one every half a metre or so, if the horizon is thick. Itâ€™s also good to grab one at the surface, and one at top of the B horizon, as the most interesting things tend to happen there (and as a result, data from those parts of the profile are often used in classification systems).\nThe result is a huge store of soil data that only â€˜existsâ€™ for part of each profile. I might have pH values for 0-10cm, 20-30, 50-60, 80-90, and 110-120, but I only have data for those depth slices. This makes it difficult to compare profiles from different locations, and it makes environmental modelling almost impossible.\nThe standard solution is to use a mass-preserving spline to interpolate between the available data, and produce estimates of mean values for continuous depth sections down the profile. The idea entered the scientific literature with Bishop, McBratney and Laslettâ€™s 1999 paper Modelling soil attribute depth functions with equal-area quadratic smoothing splines and became standard practice fairly quickly. In the mid-2000â€™s the CSIRO-funded Australian Collaborative Land Evaluation Program (ACLEP) team released a standalone app to do the job, I suspect in response to too many homebrew implementations floating around. The app made certain that everyone doing splining would get the same results from a given dataset, and this was a big deal as the drive was on to produce unified national datasets like the Australian Soil Resource Information System (ASRIS) and, later, the Soil and Landscape Grid of Australia.\n\n\n\n\n\nThe standalone app is still available from the ASRIS website, but ACLEP and ASRIS are sadly underloved these days and I donâ€™t know how much longer theyâ€™ll be around. The app itself hasnâ€™t been updated since ~2012 - the authors may have jinxed themselves by promising regular updates in the metadata :P.\nLuckily, the core functionality of SplineTool has been replicated in R, with GSIF::mpsline. That means all I had to do is wrap that function up in a web-app interface that mimics the existing tool. â€˜Hello Worldâ€™, indeed.\nThe webapp is now online at https://obrl-soil.shinyapps.io/splineapp/, so check it out and let me know what you think. Hopefully its of use to people who canâ€™t run the existing app, or donâ€™t want to learn R just to get this one task done. It has all the original app features, except for RMSE and â€˜singles reportsâ€™, which mpspline doesnâ€™t produce. To make up for it, you can view outputs as well as inputs by site, save plots, and either download .csv outputs or an .rds containing the complete object output by mpspline.\n\n\n\n\n\nRead on if you care about how I got it workingâ€¦\n\n\nI allowed myself a week to do this, and spentâ€¦ probably a solid 24 hours of that on the app, mostly because I have no self control. At least half of that was dicking around with the UI styling, I must admit, but there was still a fairly steep learning curve to negotiate.\nI went in to this with intermediate R skills and pretty basic html/css - Iâ€™d played around with making websites as a teenager mumble years ago, and then did the first few modules of freecodecampâ€™s course back in March before getting distracted and wandering off. The basic knowledge of Bootstrap I picked up there really helped, though.\nThe offical documentation and tutorials for Shiny are very good, so just working through them step by step got me most of the way there. For the rest, StackOverflow generally came to the rescue. This question about users adding to a list of values helped me implement custom output depth ranges, and this one got me a â€˜save plotsâ€™ option, which the original app didnâ€™t have.\nThereâ€™s still a few things I couldnâ€™t manage to crack, notably the ability to handle more flexible inputs. I wanted to be able to get the user to identify the input columns appropriately, rather than relying on a strictly formatted input dataset. Being able to upload a file with multiple attribute columns and then pick which to spline would have been nice. Oh well, thereâ€™s always version 2.0â€¦ jinx!\nThe source code is on my github, if you have any ideas for improvement Iâ€™d love to hear them."
  },
  {
    "objectID": "posts/2017-10-22_learning-shiny/index.html#background",
    "href": "posts/2017-10-22_learning-shiny/index.html#background",
    "title": "Learning Shiny with the Spline Tool",
    "section": "",
    "text": "My current project is about to produce a Giant Heap of data for end users to play with, and Iâ€™m concerned that it might be a bit overwhelming to digest. Even Iâ€™m having trouble trawling through it all to make sure everything is correct. A web app that allows the user to drill into that heap and just pull out what they need may be necessaryâ€¦better learn how to build one, I guess!\nIâ€™ve done just about everything else for the project in R, so I figured Iâ€™d maintain consistency and learn Shiny. As a bit of a â€˜Hello Worldâ€™ project, I decided to try and replicate a small standalone app used by soil scientists to pre-process soil laboratory data.\nSoil lab data is collected on a sample basis: you dig your hole, you grab ~200-500g of soil within a set of given depth ranges, you bag the samples up, and send them to the lab. Budget and time constraints generally mean that you donâ€™t get to sample every depth interval in a profile, so you must attempt to pick representative depth ranges. Best practice is one sample per horizon and/or one every half a metre or so, if the horizon is thick. Itâ€™s also good to grab one at the surface, and one at top of the B horizon, as the most interesting things tend to happen there (and as a result, data from those parts of the profile are often used in classification systems).\nThe result is a huge store of soil data that only â€˜existsâ€™ for part of each profile. I might have pH values for 0-10cm, 20-30, 50-60, 80-90, and 110-120, but I only have data for those depth slices. This makes it difficult to compare profiles from different locations, and it makes environmental modelling almost impossible.\nThe standard solution is to use a mass-preserving spline to interpolate between the available data, and produce estimates of mean values for continuous depth sections down the profile. The idea entered the scientific literature with Bishop, McBratney and Laslettâ€™s 1999 paper Modelling soil attribute depth functions with equal-area quadratic smoothing splines and became standard practice fairly quickly. In the mid-2000â€™s the CSIRO-funded Australian Collaborative Land Evaluation Program (ACLEP) team released a standalone app to do the job, I suspect in response to too many homebrew implementations floating around. The app made certain that everyone doing splining would get the same results from a given dataset, and this was a big deal as the drive was on to produce unified national datasets like the Australian Soil Resource Information System (ASRIS) and, later, the Soil and Landscape Grid of Australia.\n\n\n\n\n\nThe standalone app is still available from the ASRIS website, but ACLEP and ASRIS are sadly underloved these days and I donâ€™t know how much longer theyâ€™ll be around. The app itself hasnâ€™t been updated since ~2012 - the authors may have jinxed themselves by promising regular updates in the metadata :P.\nLuckily, the core functionality of SplineTool has been replicated in R, with GSIF::mpsline. That means all I had to do is wrap that function up in a web-app interface that mimics the existing tool. â€˜Hello Worldâ€™, indeed.\nThe webapp is now online at https://obrl-soil.shinyapps.io/splineapp/, so check it out and let me know what you think. Hopefully its of use to people who canâ€™t run the existing app, or donâ€™t want to learn R just to get this one task done. It has all the original app features, except for RMSE and â€˜singles reportsâ€™, which mpspline doesnâ€™t produce. To make up for it, you can view outputs as well as inputs by site, save plots, and either download .csv outputs or an .rds containing the complete object output by mpspline.\n\n\n\n\n\nRead on if you care about how I got it workingâ€¦\n\n\nI allowed myself a week to do this, and spentâ€¦ probably a solid 24 hours of that on the app, mostly because I have no self control. At least half of that was dicking around with the UI styling, I must admit, but there was still a fairly steep learning curve to negotiate.\nI went in to this with intermediate R skills and pretty basic html/css - Iâ€™d played around with making websites as a teenager mumble years ago, and then did the first few modules of freecodecampâ€™s course back in March before getting distracted and wandering off. The basic knowledge of Bootstrap I picked up there really helped, though.\nThe offical documentation and tutorials for Shiny are very good, so just working through them step by step got me most of the way there. For the rest, StackOverflow generally came to the rescue. This question about users adding to a list of values helped me implement custom output depth ranges, and this one got me a â€˜save plotsâ€™ option, which the original app didnâ€™t have.\nThereâ€™s still a few things I couldnâ€™t manage to crack, notably the ability to handle more flexible inputs. I wanted to be able to get the user to identify the input columns appropriately, rather than relying on a strictly formatted input dataset. Being able to upload a file with multiple attribute columns and then pick which to spline would have been nice. Oh well, thereâ€™s always version 2.0â€¦ jinx!\nThe source code is on my github, if you have any ideas for improvement Iâ€™d love to hear them."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Research Notes Setup 2023\n\n\n\n\n\nSecond brain? I barely have a first brain!\n\n\n\n\n\nMar 28, 2023\n\n\nLauren Oâ€™Brien\n\n\n\n\n\n\n\nWebsite update\n\n\n\n\n\nIn which the author learns a few new tricks\n\n\n\n\n\nOct 22, 2022\n\n\nLauren Oâ€™Brien\n\n\n\n\n\n\n\nNew R package: slga\n\n\n\n\n\nsoils data for the people\n\n\n\n\n\nDec 31, 2018\n\n\nLauren Oâ€™Brien\n\n\n\n\n\n\n\nNew R package: h3jsr\n\n\n\n\n\nProbably should have learned more JavaScript instead &gt;.&gt;\n\n\n\n\n\nDec 21, 2018\n\n\nLauren Oâ€™Brien\n\n\n\n\n\n\n\nPlaying with the Fitbit API in R\n\n\n\n\n\nIn which the author uses the FitBit API to live a more quantified life\n\n\n\n\n\nAug 1, 2018\n\n\nLauren Oâ€™Brien\n\n\n\n\n\n\n\ndsmartr: My first R package\n\n\n\n\n\nand some thoughts on learning to code\n\n\n\n\n\nJan 16, 2018\n\n\nLauren Oâ€™Brien\n\n\n\n\n\n\n\nLearning Shiny with the Spline Tool\n\n\n\n\n\nIn which the author writes an app\n\n\n\n\n\nOct 22, 2017\n\n\nLauren Oâ€™Brien\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Blog"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Lauren O'Brien",
    "section": "",
    "text": "Iâ€™m a pedologist working for Manaaki Whenua - Landcare Research (MWLR) in the North Island of Aotearoa New Zealand. My primary focuses are expanding S-Map coverage, supporting soil research activities and training the next generation of MWLR pedologists. I have strong interests in soil description, classification standards, and informatics. I also program (mostly in R) with a focus on geospatial technologies.\nI keep a technical blog on this site.\n\n\n\n\n\n\n\nQuarto-fied New Zealand Soil Classification\nQuarto-fied World Reference Base for Soil Resources\nShiny web app for splining soil data (source code)\n\n\n\n\n\nh3jsr: Access Uberâ€™s h3-js library via R and V8\nmpspline2: A mass-preserving spline function for soils data\nem38: work with Geonics EM38 data files in R",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#hi",
    "href": "about.html#hi",
    "title": "Lauren O'Brien",
    "section": "",
    "text": "Iâ€™m a pedologist working for Manaaki Whenua - Landcare Research (MWLR) in the North Island of Aotearoa New Zealand. My primary focuses are expanding S-Map coverage, supporting soil research activities and training the next generation of MWLR pedologists. I have strong interests in soil description, classification standards, and informatics. I also program (mostly in R) with a focus on geospatial technologies.\nI keep a technical blog on this site.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "about.html#selected-projects",
    "href": "about.html#selected-projects",
    "title": "Lauren O'Brien",
    "section": "",
    "text": "Quarto-fied New Zealand Soil Classification\nQuarto-fied World Reference Base for Soil Resources\nShiny web app for splining soil data (source code)\n\n\n\n\n\nh3jsr: Access Uberâ€™s h3-js library via R and V8\nmpspline2: A mass-preserving spline function for soils data\nem38: work with Geonics EM38 data files in R",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Recent posts",
    "section": "",
    "text": "Research Notes Setup 2023\n\n\n\n\n\nSecond brain? I barely have a first brain!\n\n\n\n\n\nMar 28, 2023\n\n\nLauren Oâ€™Brien\n\n\n\n\n\n\n\nWebsite update\n\n\n\n\n\nIn which the author learns a few new tricks\n\n\n\n\n\nOct 22, 2022\n\n\nLauren Oâ€™Brien\n\n\n\n\n\n\n\nNew R package: slga\n\n\n\n\n\nsoils data for the people\n\n\n\n\n\nDec 31, 2018\n\n\nLauren Oâ€™Brien\n\n\n\n\n\n\nNo matching items\n\n\n\nRecent fieldwork",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "posts/2018-01-16_dsmartr-announcement/index.html",
    "href": "posts/2018-01-16_dsmartr-announcement/index.html",
    "title": "dsmartr: My first R package",
    "section": "",
    "text": "Welp, Iâ€™ve written my first R package so I guess I oughta blog about it. dsmartr is a digital soils mapping package which implements the DSMART algorithm of Odgers et al (2014). The idea is to take existing polygon-based soils mapping and correlate it with various dense gridded environmental datasets (covariates), allowing production of a more detailed soils map. â€˜Soil map disaggregationâ€™ is a common phrase used to describe the process. Iâ€™m going to demonstrate the packageâ€™s use in a series of future blog posts, but today I want to stay meta and talk about how I got to this point in the first place.\nIn mid-2016 I was handed a work project that had two goals: 1) Use DSMART to disaggregate a whole lot of 1:100,000 scale soils mapping along the Queensland coast, and 2) use the disaggregated outputs as a basis for mapping soils attributes and constraints to agriculture at a fine scale (30m pixels). If the process worked well enough, it could put property-scale soils mapping in the hands of local landowners without the need for a lot of extra field and lab-work. If the process didnâ€™t work that well, it would be a clear demonstration that we can only push legacy soils data so far.\nI came into this with fairly minimal Python and R skills - Iâ€™d done some 101-level courses and mucked around with basic scripting, and I had a strong background in point-and-click GIS software (oh, and soil science, of course!). That alone might have got me through to some acceptable products without too much fuss, but Iâ€™d been reading a lot about reproducible workflows and had a bee in my bonnet about making the project fully open-source and replicable. Point-and-click was out of the question, I had to up my scripting game.\nMy learning from this point on was frustration-driven and somewhat chaotic, but since it gave me a lot of small, clear goals to work on, I think it was actually a good way to learn deeply. A lot of my intial upskilling was centered around input data preparation, which gave me a solid grounding in the R packages responsible for spatial data and database interaction. Tasks included:\n\nConnecting to our Oracle-based soils database to extract both spatial and attribute data (skills: prying non-standard connection details out of database admin staff (â€˜â€¦Why canâ€™t you just use Discoverer?â€™ â€˜SO. MANY. REASONS.â€™); using DBI; writing SQL in R scripts)\nCleaning the input map data and arranging it in a format suitable for disaggregation (skills: cleaning spatial data; combining multiple adjacent soils map projects into one coherent dataset; rearranging and reformatting attributes)\nFinding and downloading environmental covariates from internal and external repositories (skills: finding out about covariates from the literature, conversations with colleagues, strategic googling; using GDAL to extract portions of large (e.g.Â Australia-sized) rasters stored in online repositories; appeasing my &*%^ing workplace firewall)\nCreating new covariates using some of the downloaded data and a wide range of existing algorithms (skills: Using open source GIS tools like GRASS and SAGA via R with raster, rgrass7, and good olâ€™ base::system2(); picking up enough OS knowledge to get all those programs to talk to each other).\n\nOther skills I picked up in this stage related to using RStudio to properly document my work. I started using R projects to manage each sub-area of my project, as the data had to be processed in geographically distinct sections. I moved from .R scripts to .Rmds, generating low-level reporting for each processing step I took from raw data to final product. I also started adding tables and graphs to my Rmd reports with packages like DT, ggplot2, and mapview. The process of learning to write these reports was incredibly valuable as it forced me to work out a solid structure for my data analysis process - one that worked both for me and for the less-technical people who were funding my project and supervising my work.\nAs I was developing these processes, I started to look more closely at the DSMART code I was using. The DSMART algorithm was originally implemented in Python, and later ported to R (non-CRAN). I had started my project using the Python version because â€˜Everybody Says Python Is Betterâ€™. In this caseâ€¦ nope. I switched to R after realising the Py code couldnâ€™t handle the data volume I was throwing at it. Also, Iâ€™d spent a couple of weeks battling obscure dependencies and compatibility issues that had arisen as the package aged, and I was heartily sick of it. I could have persisted trying to learn Py for this work, but R was just far more accessible. Better docs, more tutorials, and easier setup. CRAN really is amazing.\nAnyway, the existing R version was more stable, if slightly slower, but still didnâ€™t quite meet my needsâ€¦so I started tinkering, and things kind of snowballed from there. The R package I was using had some bugs and RAM-consumption issues that needed attention. I started small, fixing the bugs and tweaking some output files, and before long I had alternate versions of the main package functions that could handle much bigger input datasets without falling over. This was a huge confidence booster, so when the sf package was released as the successor to sp, I felt capable of modifying the code to take advantage of the newer package. This led to substantial speed gains and more readable code, although unfortunately I couldnâ€™t drop sp completely as raster is not sf-compatible. At around this point (late 2016), I was achieving good enough outputs with my pilot study area to present my work at the joint Aus/NZ soils conference in Christchurch.\nAfter that I decided I should go ahead and package my functions rather than relying on standalone scripts. Colleagues in my office wanted to use my scripts, and getting them to run on multiple machines without my supervision was difficult. After having spent all that time learning to code and applying my fancy new skills, I was having to rush through disaggregating the rest of the soils maps on my list, and I still had to get a good process for attribute mapping off the ground. I didnâ€™t need distractions! I also wanted a packaged version of my code to make it easy to demonstrate exactly which code Iâ€™d used for my project, and so I could cite it clearly.\nStarting the packaging process was very easy with the help of github and R packages. Quiet shout-out to the GitHub for Windows desktop app, incidentally. Getting version control set up was the easy part, though - going from scripts to packaged functions involved a lot more code modification than I thought.\nMy functions used a lot of tidyverse-style piped code which doesnâ€™t really work inside packages without substantial modification, and Iâ€™d gotten a bit caught up using fancy new functions where base R was perfectly fine. I love tidyverse for data prep and display, but I donâ€™t fully grok programming tidyverse-style yet and I confess Iâ€™m not in a huge hurry to learn.\nI then learnt how to document my R code properly using roxygen2 and actually spent a lot of time on that - Iâ€™ve often been frustrated by other packagesâ€™ docs and couldnâ€™t bear to be a hypocrite on that front. I also added quite a few functions to handle data preparation and post-processing evaluation. Draft version 0.0.0.9013 wound up being the version I used to finalise my disaggregation project in early November 2017, as I was over deadline and also quite burnt out after 18 months of very steep learning curve.\nThatâ€™s probably the downside of learning the way I did - I got into this spiral of learning some new trick, immediately having to try it out, and then if it worked, having to re-run my code for half a dozen sub-project-areas so that all the outputs were consistent. This was time-consuming and stressful even though it did lead to stronger products, and it was difficult for me to draw a line and say â€˜enoughâ€™. Eventually I admitted how exhausted I was getting and forced myself to wrap things up. At that point my outputs werenâ€™t going to get any better, and I was basically running on Berocca and spite. D-minus work/life balance management, would not reccommend.\nIâ€™ve since recharged enough to progress the package to something Iâ€™m content to publicise - the last task was to unit test as much as possible. The main functions are now just wrappers that handle things like on-disk file creation and parallel processing, whereas before they contained a lot of sub-functions that did the actual work of disaggregation. The wrappers still make life a lot easier for the end user, but the important parts of the process like polygon sampling are now separated, documented and covered by unit tests so its clear to everyone that they do what they ought to. Iâ€™m not actually sure how best to extend unit test coverage beyond where it is now (ideas welcome!), but Iâ€™m confident that the package works as it should.\n\n\nThroughout this project Iâ€™ve been very reliant on free online learning resources, as not many people in my workplace do any kind of programming, and there was no funding for formal training. Resources I found invaluable include:\n\nTwitter: Twitter is amazing for #rstats. Following the right people and hashtags kept me up to date with everything from Big Stuff like new R packages to useful little tips like â€˜use file.path() instead of paste0()â€™. The only downside is managing the onslaught of new ideas; this is a very fast-moving space.\nrweekly.org has become a fantastic digest of all things #rstats.\nGIS-SE and Stack Overflow - I know SE sites have an intimidating reputation, but its still worth wading in, you just have to pay attention to the social norms there. They do exist for a reason, which others have mentioned but bears repeating: I solved at least half a dozen major problems I was having without even asking a question. The act of trying to formulate an acceptable question that wouldnâ€™t get locked led me straight to the solution. Better than a rubber duck. Iâ€™d like to think Iâ€™ve given back a bit, too.\nEdzer Pebesmaâ€™s vignettes for sf, and his blog posts on r-spatial were invaluable for the move from sp to sf, and to gaining a deeper understanding of how spatial data stuff really works. This is something that a lot of point-and-click GIS users donâ€™t even realise they donâ€™t understand - Iâ€™ve had some uncomfortable Dunning-Kruger moments this past year, but Iâ€™m better off for it.\nGitHub issues pages - if your problem is not on SE, its probably here.\nHadley Wickhamâ€™s online books â€˜R for Data Scienceâ€™ and â€˜R packagesâ€™ (especially the latter). I know, me and everyone else, but theyâ€™re popular for a reason.\nYihui Xieâ€™s knitr documentation: https://yihui.name/knitr/options/#chunk_options may as well have been my homepage for while there.\nKieran Healyâ€™s â€˜Data Visualisation: A practical introductionâ€™. This is just so well-written, it doesnâ€™t matter that its not aimed at my field. It works well as a general intro to R and to best-practice data visualisation, as well as providing specific coding instruction.\n\nHonourable mention for the upcoming â€˜Geocomputation with Râ€™ ebook, which came along a little too late for me, but is worth a read for anyone new to this space. Itâ€™ll save you a lot of time. The RStudio community forums also launched recently and theyâ€™re pretty cool.\n\n\n\n\nAt some point I should consider a CRAN submission, but that can wait until the various versions of R-based DSMART code have been reconciled. I donâ€™t think anyone wants multiple versions of the same idea on CRAN, and Iâ€™ve had a few idle chats with Nathan Odgers and Brendan Malone about combining our efforts. Herding soil scientists is worse than herding cats though, so donâ€™t hold your breath :P\nOh gosh stars is coming! I might be able to move away from sp/raster later this year, which will be nice as dsmartrâ€™s dependency list is quite long. Not going to bother until stars is on CRAN, though.\nTrain myself to type â€˜dsmartrâ€™ correctly the first time instead of having to constantly change it from dsamrtr *sigh*\n\nOk, Iâ€™ve rambled enough. Next time, how to DSMART."
  },
  {
    "objectID": "posts/2018-01-16_dsmartr-announcement/index.html#background",
    "href": "posts/2018-01-16_dsmartr-announcement/index.html#background",
    "title": "dsmartr: My first R package",
    "section": "",
    "text": "Welp, Iâ€™ve written my first R package so I guess I oughta blog about it. dsmartr is a digital soils mapping package which implements the DSMART algorithm of Odgers et al (2014). The idea is to take existing polygon-based soils mapping and correlate it with various dense gridded environmental datasets (covariates), allowing production of a more detailed soils map. â€˜Soil map disaggregationâ€™ is a common phrase used to describe the process. Iâ€™m going to demonstrate the packageâ€™s use in a series of future blog posts, but today I want to stay meta and talk about how I got to this point in the first place.\nIn mid-2016 I was handed a work project that had two goals: 1) Use DSMART to disaggregate a whole lot of 1:100,000 scale soils mapping along the Queensland coast, and 2) use the disaggregated outputs as a basis for mapping soils attributes and constraints to agriculture at a fine scale (30m pixels). If the process worked well enough, it could put property-scale soils mapping in the hands of local landowners without the need for a lot of extra field and lab-work. If the process didnâ€™t work that well, it would be a clear demonstration that we can only push legacy soils data so far.\nI came into this with fairly minimal Python and R skills - Iâ€™d done some 101-level courses and mucked around with basic scripting, and I had a strong background in point-and-click GIS software (oh, and soil science, of course!). That alone might have got me through to some acceptable products without too much fuss, but Iâ€™d been reading a lot about reproducible workflows and had a bee in my bonnet about making the project fully open-source and replicable. Point-and-click was out of the question, I had to up my scripting game.\nMy learning from this point on was frustration-driven and somewhat chaotic, but since it gave me a lot of small, clear goals to work on, I think it was actually a good way to learn deeply. A lot of my intial upskilling was centered around input data preparation, which gave me a solid grounding in the R packages responsible for spatial data and database interaction. Tasks included:\n\nConnecting to our Oracle-based soils database to extract both spatial and attribute data (skills: prying non-standard connection details out of database admin staff (â€˜â€¦Why canâ€™t you just use Discoverer?â€™ â€˜SO. MANY. REASONS.â€™); using DBI; writing SQL in R scripts)\nCleaning the input map data and arranging it in a format suitable for disaggregation (skills: cleaning spatial data; combining multiple adjacent soils map projects into one coherent dataset; rearranging and reformatting attributes)\nFinding and downloading environmental covariates from internal and external repositories (skills: finding out about covariates from the literature, conversations with colleagues, strategic googling; using GDAL to extract portions of large (e.g.Â Australia-sized) rasters stored in online repositories; appeasing my &*%^ing workplace firewall)\nCreating new covariates using some of the downloaded data and a wide range of existing algorithms (skills: Using open source GIS tools like GRASS and SAGA via R with raster, rgrass7, and good olâ€™ base::system2(); picking up enough OS knowledge to get all those programs to talk to each other).\n\nOther skills I picked up in this stage related to using RStudio to properly document my work. I started using R projects to manage each sub-area of my project, as the data had to be processed in geographically distinct sections. I moved from .R scripts to .Rmds, generating low-level reporting for each processing step I took from raw data to final product. I also started adding tables and graphs to my Rmd reports with packages like DT, ggplot2, and mapview. The process of learning to write these reports was incredibly valuable as it forced me to work out a solid structure for my data analysis process - one that worked both for me and for the less-technical people who were funding my project and supervising my work.\nAs I was developing these processes, I started to look more closely at the DSMART code I was using. The DSMART algorithm was originally implemented in Python, and later ported to R (non-CRAN). I had started my project using the Python version because â€˜Everybody Says Python Is Betterâ€™. In this caseâ€¦ nope. I switched to R after realising the Py code couldnâ€™t handle the data volume I was throwing at it. Also, Iâ€™d spent a couple of weeks battling obscure dependencies and compatibility issues that had arisen as the package aged, and I was heartily sick of it. I could have persisted trying to learn Py for this work, but R was just far more accessible. Better docs, more tutorials, and easier setup. CRAN really is amazing.\nAnyway, the existing R version was more stable, if slightly slower, but still didnâ€™t quite meet my needsâ€¦so I started tinkering, and things kind of snowballed from there. The R package I was using had some bugs and RAM-consumption issues that needed attention. I started small, fixing the bugs and tweaking some output files, and before long I had alternate versions of the main package functions that could handle much bigger input datasets without falling over. This was a huge confidence booster, so when the sf package was released as the successor to sp, I felt capable of modifying the code to take advantage of the newer package. This led to substantial speed gains and more readable code, although unfortunately I couldnâ€™t drop sp completely as raster is not sf-compatible. At around this point (late 2016), I was achieving good enough outputs with my pilot study area to present my work at the joint Aus/NZ soils conference in Christchurch.\nAfter that I decided I should go ahead and package my functions rather than relying on standalone scripts. Colleagues in my office wanted to use my scripts, and getting them to run on multiple machines without my supervision was difficult. After having spent all that time learning to code and applying my fancy new skills, I was having to rush through disaggregating the rest of the soils maps on my list, and I still had to get a good process for attribute mapping off the ground. I didnâ€™t need distractions! I also wanted a packaged version of my code to make it easy to demonstrate exactly which code Iâ€™d used for my project, and so I could cite it clearly.\nStarting the packaging process was very easy with the help of github and R packages. Quiet shout-out to the GitHub for Windows desktop app, incidentally. Getting version control set up was the easy part, though - going from scripts to packaged functions involved a lot more code modification than I thought.\nMy functions used a lot of tidyverse-style piped code which doesnâ€™t really work inside packages without substantial modification, and Iâ€™d gotten a bit caught up using fancy new functions where base R was perfectly fine. I love tidyverse for data prep and display, but I donâ€™t fully grok programming tidyverse-style yet and I confess Iâ€™m not in a huge hurry to learn.\nI then learnt how to document my R code properly using roxygen2 and actually spent a lot of time on that - Iâ€™ve often been frustrated by other packagesâ€™ docs and couldnâ€™t bear to be a hypocrite on that front. I also added quite a few functions to handle data preparation and post-processing evaluation. Draft version 0.0.0.9013 wound up being the version I used to finalise my disaggregation project in early November 2017, as I was over deadline and also quite burnt out after 18 months of very steep learning curve.\nThatâ€™s probably the downside of learning the way I did - I got into this spiral of learning some new trick, immediately having to try it out, and then if it worked, having to re-run my code for half a dozen sub-project-areas so that all the outputs were consistent. This was time-consuming and stressful even though it did lead to stronger products, and it was difficult for me to draw a line and say â€˜enoughâ€™. Eventually I admitted how exhausted I was getting and forced myself to wrap things up. At that point my outputs werenâ€™t going to get any better, and I was basically running on Berocca and spite. D-minus work/life balance management, would not reccommend.\nIâ€™ve since recharged enough to progress the package to something Iâ€™m content to publicise - the last task was to unit test as much as possible. The main functions are now just wrappers that handle things like on-disk file creation and parallel processing, whereas before they contained a lot of sub-functions that did the actual work of disaggregation. The wrappers still make life a lot easier for the end user, but the important parts of the process like polygon sampling are now separated, documented and covered by unit tests so its clear to everyone that they do what they ought to. Iâ€™m not actually sure how best to extend unit test coverage beyond where it is now (ideas welcome!), but Iâ€™m confident that the package works as it should.\n\n\nThroughout this project Iâ€™ve been very reliant on free online learning resources, as not many people in my workplace do any kind of programming, and there was no funding for formal training. Resources I found invaluable include:\n\nTwitter: Twitter is amazing for #rstats. Following the right people and hashtags kept me up to date with everything from Big Stuff like new R packages to useful little tips like â€˜use file.path() instead of paste0()â€™. The only downside is managing the onslaught of new ideas; this is a very fast-moving space.\nrweekly.org has become a fantastic digest of all things #rstats.\nGIS-SE and Stack Overflow - I know SE sites have an intimidating reputation, but its still worth wading in, you just have to pay attention to the social norms there. They do exist for a reason, which others have mentioned but bears repeating: I solved at least half a dozen major problems I was having without even asking a question. The act of trying to formulate an acceptable question that wouldnâ€™t get locked led me straight to the solution. Better than a rubber duck. Iâ€™d like to think Iâ€™ve given back a bit, too.\nEdzer Pebesmaâ€™s vignettes for sf, and his blog posts on r-spatial were invaluable for the move from sp to sf, and to gaining a deeper understanding of how spatial data stuff really works. This is something that a lot of point-and-click GIS users donâ€™t even realise they donâ€™t understand - Iâ€™ve had some uncomfortable Dunning-Kruger moments this past year, but Iâ€™m better off for it.\nGitHub issues pages - if your problem is not on SE, its probably here.\nHadley Wickhamâ€™s online books â€˜R for Data Scienceâ€™ and â€˜R packagesâ€™ (especially the latter). I know, me and everyone else, but theyâ€™re popular for a reason.\nYihui Xieâ€™s knitr documentation: https://yihui.name/knitr/options/#chunk_options may as well have been my homepage for while there.\nKieran Healyâ€™s â€˜Data Visualisation: A practical introductionâ€™. This is just so well-written, it doesnâ€™t matter that its not aimed at my field. It works well as a general intro to R and to best-practice data visualisation, as well as providing specific coding instruction.\n\nHonourable mention for the upcoming â€˜Geocomputation with Râ€™ ebook, which came along a little too late for me, but is worth a read for anyone new to this space. Itâ€™ll save you a lot of time. The RStudio community forums also launched recently and theyâ€™re pretty cool.\n\n\n\n\nAt some point I should consider a CRAN submission, but that can wait until the various versions of R-based DSMART code have been reconciled. I donâ€™t think anyone wants multiple versions of the same idea on CRAN, and Iâ€™ve had a few idle chats with Nathan Odgers and Brendan Malone about combining our efforts. Herding soil scientists is worse than herding cats though, so donâ€™t hold your breath :P\nOh gosh stars is coming! I might be able to move away from sp/raster later this year, which will be nice as dsmartrâ€™s dependency list is quite long. Not going to bother until stars is on CRAN, though.\nTrain myself to type â€˜dsmartrâ€™ correctly the first time instead of having to constantly change it from dsamrtr *sigh*\n\nOk, Iâ€™ve rambled enough. Next time, how to DSMART."
  },
  {
    "objectID": "posts/2018-12-21_h3jsr-announcement/index.html",
    "href": "posts/2018-12-21_h3jsr-announcement/index.html",
    "title": "New R package: h3jsr",
    "section": "",
    "text": "Discrete Global Grids! Theyâ€™re pretty cool, and slowly starting to catch on. Googleâ€™s been plugging away at S2 for a while now, and Uber recently released H3. Both libraries are open-sourced and have their own sets of interesting features, but donâ€™t seem to have found their way into traditional GIS software yet, so you need some coding skill to access them.\nI could see some interesting potential use cases for H3 as soon as I read the documentation, so I was super keen to start playing with it ASAP. There were some barriers between me and all the hexagons I could eat though, so I had to do a little work first.\nMy process here was basically:\n\nrealise that no R bindings were available for H3 (apart from this attempt, which appears to be slightly abandoned and doesnâ€™t work in Windows), sulk a little\nrealise thereâ€™s a transpiled version, h3-js, and that V8 is a thing (hot damn!)\nspend a Saturday morning figuring out how to get h3-js bundled into a v8 session\nrealise I now have to learn some damn JavaScript; spend Saturday afternoon on codecademy\nbriefly ponder whether six (6) hours of JS experience is enough to get by\nproceed anyway because Iâ€™ve got this far and f*&k imposter syndrome, right? Right.\nâ€¦?\nprofit!\n\nh3jsr is now available from GitHub. Iâ€™m feeling pretty good about it."
  },
  {
    "objectID": "posts/2018-12-21_h3jsr-announcement/index.html#background",
    "href": "posts/2018-12-21_h3jsr-announcement/index.html#background",
    "title": "New R package: h3jsr",
    "section": "",
    "text": "Discrete Global Grids! Theyâ€™re pretty cool, and slowly starting to catch on. Googleâ€™s been plugging away at S2 for a while now, and Uber recently released H3. Both libraries are open-sourced and have their own sets of interesting features, but donâ€™t seem to have found their way into traditional GIS software yet, so you need some coding skill to access them.\nI could see some interesting potential use cases for H3 as soon as I read the documentation, so I was super keen to start playing with it ASAP. There were some barriers between me and all the hexagons I could eat though, so I had to do a little work first.\nMy process here was basically:\n\nrealise that no R bindings were available for H3 (apart from this attempt, which appears to be slightly abandoned and doesnâ€™t work in Windows), sulk a little\nrealise thereâ€™s a transpiled version, h3-js, and that V8 is a thing (hot damn!)\nspend a Saturday morning figuring out how to get h3-js bundled into a v8 session\nrealise I now have to learn some damn JavaScript; spend Saturday afternoon on codecademy\nbriefly ponder whether six (6) hours of JS experience is enough to get by\nproceed anyway because Iâ€™ve got this far and f*&k imposter syndrome, right? Right.\nâ€¦?\nprofit!\n\nh3jsr is now available from GitHub. Iâ€™m feeling pretty good about it."
  },
  {
    "objectID": "posts/2018-12-21_h3jsr-announcement/index.html#y-tho",
    "href": "posts/2018-12-21_h3jsr-announcement/index.html#y-tho",
    "title": "New R package: h3jsr",
    "section": "Y tho",
    "text": "Y tho\nRight now my own applications for this package are nice data aggregation and pretty maps. That might seem basic, but that does seem to be all that Uber are using it for themselves so far, and its solved some substantial business problems."
  },
  {
    "objectID": "posts/2018-12-21_h3jsr-announcement/index.html#performance",
    "href": "posts/2018-12-21_h3jsr-announcement/index.html#performance",
    "title": "New R package: h3jsr",
    "section": "Performance",
    "text": "Performance\nYouâ€™ll be able to do a fair bit with this package so long as you think ahead. The most important thing to remember is that every call to a h3jsr function involves casting data into a JS environment via JSON, and that eats time. Aim to feed as much data into one function call as possible - use lists, vectors or dataframes as input wherever you can, donâ€™t try and iterate over individual geometries. Bear in mind that thereâ€™s an upper limit to what V8 can transfer in one hit."
  },
  {
    "objectID": "posts/2018-12-21_h3jsr-announcement/index.html#demo-time",
    "href": "posts/2018-12-21_h3jsr-announcement/index.html#demo-time",
    "title": "New R package: h3jsr",
    "section": "Demo time",
    "text": "Demo time\nI did the bulk of the work on this package back in July, and then idly tinkered with it while failing to complete this post - my examples were boring! Luckily the Uber Open Summit 2018 happened not long ago and as part of it, h3-js dev Nick Rabinowitz ran a great live tutorial on Suitability Analysis using h3-js and Mapbox GL JS. Attempting a rebuild in R seems like a good way to demonstrate key functions.\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(geojsonsf)\nlibrary(tidyverse)\nlibrary(ggspatial)\nlibrary(raster)\nlibrary(sf)\nlibrary(h3jsr)\noptions(stringsAsFactors = FALSE)\n\nAll the tutorial inputs are github gists, so I can download and convert them to sf data frames like so:\nOakland crime reports, last 90 days. Source: data.oaklandnet.com:\n\ncrime_90_days &lt;- httr::GET('https://gist.githubusercontent.com/nrabinowitz/d3a5ca3e3e40727595dd137b65058c76/raw/f5ef0fed8972d04a27727ebb50e065265e2d853f/oakland_crime_90days.json') %&gt;%\n  httr::content() %&gt;%\n  fromJSON() %&gt;%\n  sf::st_as_sf(., coords = c('lng', 'lat'), crs = 4326) # JSON's always 4326\n\nhead(crime_90_days)\n\n## Simple feature collection with 6 features and 1 field\n## geometry type:  POINT\n## dimension:      XY\n## bbox:           xmin: -122.2758 ymin: 37.75606 xmax: -122.1889 ymax: 37.81339\n## epsg (SRID):    4326\n## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n##                   type                   geometry\n## 1            VANDALISM POINT (-122.2655 37.81339)\n## 2              ASSAULT  POINT (-122.2758 37.7969)\n## 3        THEFT/LARCENY POINT (-122.2026 37.75606)\n## 4              ROBBERY POINT (-122.2352 37.78423)\n## 5 DISTURBING THE PEACE POINT (-122.1889 37.79133)\n## 6            VANDALISM  POINT (-122.219 37.78628)\nOakland public school locations. Source: data.oaklandnet.com\n\npublic_schools &lt;- httr::GET('https://gist.githubusercontent.com/nrabinowitz/d3a5ca3e3e40727595dd137b65058c76/raw/babf7357f15c99a1b2a507a33d332a4a87b7df8d/public_schools.json') %&gt;%\n  httr::content() %&gt;%\n  fromJSON() %&gt;%\n  sf::st_as_sf(., coords = c('lng', 'lat'), crs = 4326)\n\nhead(public_schools)\n\n## Simple feature collection with 6 features and 1 field\n## geometry type:  POINT\n## dimension:      XY\n## bbox:           xmin: -122.2869 ymin: 37.74664 xmax: -122.1656 ymax: 37.813\n## epsg (SRID):    4326\n## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n##         type                   geometry\n## 1    Charter POINT (-122.1849 37.79886)\n## 2    Charter  POINT (-122.225 37.77617)\n## 3     Middle POINT (-122.1656 37.74664)\n## 4       High   POINT (-122.2869 37.813)\n## 5 Elementary   POINT (-122.23 37.77982)\n## 6 Elementary POINT (-122.2371 37.80036)\nBART station locations. Source: bart.gov. This is GeoJSON data, not straight JSON, so note how the import proccess is a little different.\n\nbart_stations &lt;- httr::GET('https://gist.githubusercontent.com/nrabinowitz/d3a5ca3e3e40727595dd137b65058c76/raw/8f1a3e30113472404feebc288e83688a6d5cf33d/bart.json') %&gt;%\n  httr::content() %&gt;%\n  geojson_sf()\n\nhead(bart_stations[, 1])\n\n## Simple feature collection with 6 features and 1 field\n## geometry type:  POINT\n## dimension:      XYZ\n## bbox:           xmin: -122.4475 ymin: 37.72158 xmax: -122.2686 ymax: 37.8528\n## epsg (SRID):    4326\n## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n##                                  name                       geometry\n## 1 12th St. Oakland City Center (12TH) POINT Z (-122.2715 37.80377 0)\n## 2             16th St. Mission (16TH) POINT Z (-122.4197 37.76506 0)\n## 3             19th St. Oakland (19TH) POINT Z (-122.2686 37.80835 0)\n## 4             24th St. Mission (24TH) POINT Z (-122.4181 37.75247 0)\n## 5                        Ashby (ASHB)  POINT Z (-122.2701 37.8528 0)\n## 6                  Balboa Park (BALB) POINT Z (-122.4475 37.72158 0)\nTravel times from Oakland to downtown SF by census tract. Source: movement.uber.com\n\nsf_travel_times &lt;- httr::GET('https://gist.githubusercontent.com/nrabinowitz/d3a5ca3e3e40727595dd137b65058c76/raw/657a9f3b64fedc718c3882cd4adc645ac0b4cfc5/oakland_travel_times.json') %&gt;%\n  httr::content() %&gt;%\n  geojson_sf()\n\nhead(sf_travel_times)\n\n## Simple feature collection with 6 features and 3 fields\n## geometry type:  MULTIPOLYGON\n## dimension:      XY\n## bbox:           xmin: -122.3049 ymin: 37.74276 xmax: -122.1595 ymax: 37.84773\n## epsg (SRID):    4326\n## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n##   MOVEMENT_ID                                DISPLAY_NAME travelTime\n## 1          46   500 Chester Street, West Oakland, Oakland        708\n## 2          47             9700 Birch Street, Cox, Oakland       1575\n## 3          58        5600 Genoa Street, Santa Fe, Oakland       1015\n## 4          98  500 10th Street, Downtown Oakland, Oakland        826\n## 5          99 2400 19th Avenue, Highland Terrace, Oakland       1166\n## 6         151  500 20th Street, Downtown Oakland, Oakland        908\n##                         geometry\n## 1 MULTIPOLYGON (((-122.304 37...\n## 2 MULTIPOLYGON (((-122.1725 3...\n## 3 MULTIPOLYGON (((-122.2779 3...\n## 4 MULTIPOLYGON (((-122.2796 3...\n## 5 MULTIPOLYGON (((-122.238 37...\n## 6 MULTIPOLYGON (((-122.276 37...\nOakland points of interest. Source: uber.com/local\n\npois &lt;- httr::GET('https://gist.githubusercontent.com/nrabinowitz/d3a5ca3e3e40727595dd137b65058c76/raw/ded89c2acef426fe3ee59b05096ed1baecf02090/oakland-poi.json') %&gt;%\n  httr::content()  %&gt;%\n  fromJSON() %&gt;%\n  sf::st_as_sf(., coords = c('lng', 'lat'), crs = 4326) %&gt;%\n  dplyr::filter(type %in% c('Cafes', 'Places to eat', 'Restaurant'))\n\nhead(pois)\n\n## Simple feature collection with 6 features and 1 field\n## geometry type:  POINT\n## dimension:      XY\n## bbox:           xmin: -122.2763 ymin: 37.77091 xmax: -122.211 ymax: 37.83476\n## epsg (SRID):    4326\n## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n##         type                   geometry\n## 1 Restaurant  POINT (-122.2567 37.8281)\n## 2 Restaurant POINT (-122.2632 37.83476)\n## 3 Restaurant POINT (-122.2705 37.80706)\n## 4 Restaurant  POINT (-122.211 37.77091)\n## 5 Restaurant POINT (-122.2763 37.79486)\n## 6 Restaurant POINT (-122.2437 37.81062)\nThatâ€™s a lot of messy data:\n\ncropper &lt;- st_bbox(c('xmin' = -122.35, 'ymin' = 37.75,\n                     'xmax' = -122.20, 'ymax' = 37.85), crs = st_crs(4326))\nplot_these &lt;- lapply(list(sf_travel_times, crime_90_days, public_schools,\n                          bart_stations, pois), function(x) {\n                            sf::st_crop(x, cropper)\n                            })\nggplot() +\n  layer_spatial(plot_these[[1]], aes(fill = travelTime), alpha = 0.9,\n                show.legend = FALSE) +\n  scale_fill_viridis_c() +\n  layer_spatial(plot_these[[2]], col = 'red',     pch = 20) +\n  layer_spatial(plot_these[[3]], col = 'cyan',    pch = 19) +\n  layer_spatial(plot_these[[4]], col = 'yellow',  pch = 18, size = 2) + \n  layer_spatial(plot_these[[5]], col = 'magenta', pch = 17) +\n  theme_minimal() +\n  ggtitle('H3 Suitability analysis', subtitle = 'Unprocessed input data')\n\n\n\n\n\n\nTime to make some sense of it.\nWeâ€™ll take each of the raw data layers weâ€™re bringing in and convert them to hexagon layers. Each layer will be a map of H3 index to some value normalized between zero and 1 with this helper function:\n\nnormalise_layer &lt;- function(layer = NULL, b0 = FALSE) {\n  dplyr::filter(layer, !is.na(h3)) %&gt;%\n    dplyr::group_by(h3) %&gt;%\n    dplyr::summarise('weight' = sum(weight, na.rm = TRUE)) %&gt;%\n    dplyr::mutate(norm = if (b0) {\n      scales::rescale(weight, to = c(0, 1), from =  c(0, max(weight, na.rm = TRUE)))\n      } else { scales::rescale(weight, to = c(0, 1)) }) # from = range(x)\n}\n\nAnalysis is being conducted at four of H3â€™s 15 resolution levels - 7-10 - so each data layer must be binned and normalised four times. This is where purrr can be handy.\nFor crime, the output layer is just a normalised count of incidents per hex.\n\ncrime_hexes &lt;- point_to_h3(crime_90_days, seq(7, 10)) %&gt;%\n  purrr::map(., function(h3) {\n   dat &lt;- data.frame('h3' = h3, 'weight' = 1L, \n                     stringsAsFactors = FALSE) %&gt;%\n     normalise_layer()\n  })\nhead(crime_hexes[['h3_resolution_7']])\n\n## # A tibble: 6 x 3\n##   h3              weight     norm\n##   &lt;chr&gt;            &lt;int&gt;    &lt;dbl&gt;\n## 1 872830802ffffff      5 0.000486\n## 2 872830810ffffff   2060 1       \n## 3 872830811ffffff    311 0.149   \n## 4 872830812ffffff    575 0.278   \n## 5 872830813ffffff    535 0.258   \n## 6 872830814ffffff    145 0.0686\nFor schools, thereâ€™s a bit of buffering added so that addresses adjacent to those containing a school are given some weight.\n\nschool_hexes &lt;- point_to_h3(public_schools, seq(7, 10)) %&gt;%\n  purrr::map(., function(h3) {\n    # returns 7 addresses - input and neighbours \n    near_school &lt;- get_kring(h3, 1) \n    near_wts &lt;- c(1, rep(0.5, 6)) # surrounds are worth half\n    # combine and normalise\n    dat &lt;- purrr::map_dfr(near_school, function(h3) {\n      data.frame('h3' = h3, 'weight' = near_wts, stringsAsFactors = FALSE)\n    }) %&gt;%\n      normalise_layer()\n  })\nhead(school_hexes[[1]])\n\n## # A tibble: 6 x 3\n##   h3              weight   norm\n##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n## 1 872830802ffffff    2.5 0.0556\n## 2 872830806ffffff    1   0.0139\n## 3 872830810ffffff   25.5 0.694 \n## 4 872830811ffffff   14.5 0.389 \n## 5 872830812ffffff   23.5 0.639 \n## 6 872830813ffffff   22.5 0.611\nFor BART stations, the buffering is more sophisticated, with a smooth decay function implemented. This does a better job of preserving the area of influence around a station across different H3 resolutions.\n\nkm_to_radius &lt;- function(km, res) {\n  floor(km / res_length(res, units = 'km'))\n}\n\nbart_hexes &lt;- point_to_h3(bart_stations, seq(7, 10)) %&gt;%\n  purrr::map2(., seq(7,10), function(h3, res) {\n    d &lt;- km_to_radius(1, res)\n    near_bart &lt;- get_kring_list(sort(unique(h3)), d)\n    # weights are the same for every feature so just make a template\n    near_wts &lt;- purrr::map(near_bart[1], function(feature) {\n      purrr::map2(feature, seq_along(feature), function(ring, step) {\n        wt &lt;- 1 - step * 1 / (d + 1)\n        rep(wt, length(ring))\n      })\n    }) %&gt;% unlist()\n    purrr::map(near_bart, unlist) %&gt;%\n      purrr::map_dfr(., function(x) {\n        data.frame('h3' = x, 'weight' = near_wts, stringsAsFactors = FALSE)\n        }) %&gt;%\n      normalise_layer() \n      })\nhead(bart_hexes[[1]])\n\n## # A tibble: 6 x 3\n##   h3              weight  norm\n##   &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n## 1 872830810ffffff      0   0.5\n## 2 872830813ffffff      0   0.5\n## 3 872830815ffffff      0   0.5\n## 4 872830828ffffff      0   0.5\n## 5 87283082affffff      0   0.5\n## 6 87283082cffffff      0   0.5\nFor travel time, we find all of the intersecting H3 addresses for each polygon in sf_travel_times before assigning them weights based on the travelTime attribute. The center of a given address must intersect the polygon before it can be returned. Weights are negative as lower travel times are better.\n\ntravel_hexes &lt;- purrr::map(seq(7, 10), function(res) {\n  dat &lt;- polyfill(sf_travel_times, res, simple = FALSE) \n  dat &lt;- purrr::map2_dfr(as.list(dat$travelTime), \n                         dat$h3_polyfillers, \n                         function(x, y) {\n                           data.frame('h3' = y, 'weight' = 1/x)\n    }) %&gt;%\n    normalise_layer(., TRUE)\n})\nhead(travel_hexes[[1]])\n\n## # A tibble: 6 x 3\n##   h3               weight  norm\n##   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n## 1 872830802ffffff 0.00151 0.802\n## 2 872830810ffffff 0.00110 0.586\n## 3 872830811ffffff 0.00188 1    \n## 4 872830812ffffff 0.00127 0.673\n## 5 872830813ffffff 0.00137 0.727\n## 6 872830815ffffff 0.00129 0.686\nLastly, points of interest are tallied, just as crime was.\n\nfood_hexes &lt;- point_to_h3(pois, seq(7, 10)) %&gt;%\n  purrr::map(., function(h3) {\n   dat &lt;- data.frame('h3' = h3, 'weight' = 1L, \n                     stringsAsFactors = FALSE) %&gt;%\n     normalise_layer()\n  })\nhead(food_hexes[[1]])\n\n## # A tibble: 6 x 3\n##   h3              weight   norm\n##   &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt;\n## 1 872830810ffffff    851 1     \n## 2 872830811ffffff     16 0.0176\n## 3 872830812ffffff    407 0.478 \n## 4 872830813ffffff    331 0.388 \n## 5 872830814ffffff    173 0.202 \n## 6 872830816ffffff     98 0.114\nPhew! Now we can plug the data together to get some overall weights. For each resolution, the following code adds up the normalised weights all of the â€˜goodâ€™ layers, and then subtracts crime.\n\n# arrange data by resolution instead of theme\ndatnm &lt;- c('crime', 'school', 'bart', 'travel', 'food')\ndat &lt;- list(crime_hexes, school_hexes, bart_hexes,\n                  travel_hexes, food_hexes) %&gt;%\n  purrr::transpose() %&gt;%\n  purrr::map(., setNames, datnm)\n\nfinal_surfaces &lt;- purrr::map(dat, function(res) {\n  # rename cols for nicer joins\n  purrr::map2(res, datnm, function(x,y) {\n    dplyr::rename_at(x, vars(norm), funs(paste0('n_', y))) %&gt;%\n      dplyr::select(-weight)\n  }) %&gt;%\n    # condense inputs\n    purrr::reduce(., full_join, by = 'h3') %&gt;%\n    replace(is.na(.), 0) %&gt;%\n    rowwise() %&gt;%\n    dplyr::mutate(weight = \n                    sum(c(n_school, n_bart, n_travel, n_food)) - n_crime) %&gt;%\n    ungroup() %&gt;%\n    dplyr::select(h3, weight) %&gt;%\n    normalise_layer() %&gt;%\n    h3_to_polygon(., simple = FALSE)\n})\nhead(final_surfaces[[2]])\n\n## Simple feature collection with 6 features and 4 fields\n## geometry type:  POLYGON\n## dimension:      XY\n## bbox:           xmin: -122.3869 ymin: 37.79247 xmax: -122.3053 ymax: 37.8162\n## epsg (SRID):    4326\n## proj4string:    +proj=longlat +datum=WGS84 +no_defs\n##      weight       norm      h3_address h3_resolution\n## 1 0.0000000 0.02495816 882830801bfffff             8\n## 2 0.8024133 0.45593409 8828308021fffff             8\n## 3 0.8024133 0.45593409 8828308023fffff             8\n## 4 0.8024133 0.45593409 8828308025fffff             8\n## 5 1.0000000 0.56205787 8828308027fffff             8\n## 6 0.8024133 0.45593409 882830802bfffff             8\n##                         geometry\n## 1 POLYGON ((-122.3796 37.7924...\n## 2 POLYGON ((-122.3202 37.7985...\n## 3 POLYGON ((-122.3276 37.8044...\n## 4 POLYGON ((-122.3098 37.8009...\n## 5 POLYGON ((-122.3172 37.8068...\n## 6 POLYGON ((-122.3306 37.7961...\nAll that and I still canâ€™t give you a cool interactive map like the source Observable notebook, but my plot below matches up nicely. Winner!\n\ncropper &lt;- st_bbox(c('xmin' = -122.35, 'ymin' = 37.75,\n                     'xmax' = -122.20, 'ymax' = 37.85), crs = st_crs(4326))\nplot_that &lt;- st_crop(do.call(rbind, final_surfaces), cropper)\n\nggplot() +\n  layer_spatial(plot_that, aes(fill = norm), alpha = 0.9, col = NA) +\n  facet_wrap(. ~ h3_resolution)  +\n  scale_fill_gradientn(colors = c('#ffffD9', '#50BAC3', '#1A468A')) +\n  scale_x_continuous(breaks = c(-122.32, -122.27, -122.22)) +\n  ggtitle('H3 Suitability Analysis', \n          subtitle = 'Liveability at four resolutions') +\n  labs(fill = 'Weight') +\n  theme_minimal()\n\n\n\n\n\n\nIts nice to confirm that h3jsr does what it oughta, but my major take-home from this is that I really need to learn me some geospatial JS. The code is much more concise than the R equivalent. In fact, the above was worse before I took the time to revamp a couple of my wrapper functions. The JS is also very fast, and the interactive maps are a real winner. I know I could probably do something here with mapview or better yet, mapdeck and Shiny, but that would be even more code (not to mention getting it to work properly on my current blog setup, which Iâ€™m not sure is possibleâ€¦). So Iâ€™m very impressed.\nAnyway, there it is, feel free to experiment with the package, and I welcome PRs that will speed it up or improve useability. Its been a worthwhile learning exercise, and Iâ€™m using h3jsr fairly often at work. Iâ€™m pretty sure that an Rcpp-powered version of this that hooks into original-flavour H3 will be much, much faster, though, so I donâ€™t think this package will go to CRAN."
  },
  {
    "objectID": "posts/2022-10-24_site-update/index.html",
    "href": "posts/2022-10-24_site-update/index.html",
    "title": "Website update",
    "section": "",
    "text": "I let this site fall into disuse a couple of years ago. Life got busy, and then I decided to move overseas all by myself, and then a pandemic happened, you know, the usual.\nLately Iâ€™ve been feeling like I want to start writing again1. Thing is, I hadnâ€™t touched the blog in so long I could barely remember how to update it. I knew there was a simpler way, so Iâ€™ve bitten the bullet and converted to Quarto.\nI found moving to a Quarto website to be a fairly simple process thanks to the excellent documentation, linked examples (Mike Mahoney, Bea Milz) and Danielle Navarroâ€™s blog post on migrating from distill (as well as her blog source code, of course). That said, I had a head start on Quarto generally as Iâ€™ve already been using it for standalone files, a book conversion, and a slide deck or two. I did this whole conversion in a day, and that wouldnâ€™t have happened if this was my first attempt with Quarto."
  },
  {
    "objectID": "posts/2022-10-24_site-update/index.html#footnotes",
    "href": "posts/2022-10-24_site-update/index.html#footnotes",
    "title": "Website update",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nto be clear: this is insane of me, life hasnâ€™t gotten less busyâ€¦â†©ï¸"
  }
]